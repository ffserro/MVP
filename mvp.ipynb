{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "269fe9a4",
      "metadata": {
        "id": "269fe9a4"
      },
      "source": [
        "[![Abra no Colab](https://raw.githubusercontent.com/ffserro/MVP/master/abra-no-colab.png)](https://colab.research.google.com/github/ffserro/MVP/blob/master/mvp.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wL7a42D2_9De",
      "metadata": {
        "id": "wL7a42D2_9De"
      },
      "source": [
        "# Regressão Linear para Series Temporais - Planejamento dos dispêndios de alimentação de militares da Marinha do Brasil"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jkWGTW6eI5k9",
      "metadata": {
        "id": "jkWGTW6eI5k9"
      },
      "source": [
        "## Definição do Problema\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JgQOsnRyiCbK",
      "metadata": {
        "id": "JgQOsnRyiCbK"
      },
      "source": [
        "<div align=\"justify\">\n",
        "\n",
        "### O que é o municiamento?\n",
        "A Gestoria de Munciamento é o conjunto dos processos responsáveis por gerir diariamente a alimentação e subsistência dos militares e servidores lotados nas organizações militares da Marinha do Brasil.\n",
        "\n",
        "As principais atividades da gestoria de municiamento são:\n",
        "- Planejamento e aquisição de gêneros alimentícios\n",
        "- Controle de estoque de gêneros alimentícios\n",
        "- Escrituração e pagamento\n",
        "- Prestação de contas\n",
        "\n",
        "\n",
        "### Orçamento público e alimentação de militares\n",
        "Por ser custeada com recursos do orçamento da União, a gestoria de municiamento se submete a um sistema rigoroso de planejamento, controle, fiscalização e transparência, para garantir que os valores sejam utilizados de forma eficiente, econômica e legal.\n",
        "\n",
        "A compra de gêneros alimentícios é uma despesa recorrente e significativa. Uma gestão eficiente garante que os recursos financeiros sejam alocados de forma inteligente, evitando gastos desnecessários.\n",
        "\n",
        "Uma boa gestão de estoques minimiza desperdícios de alimentos por validade ou má conservação.\n",
        "\n",
        "Como em qualquer gasto público, a aquisição de suprimentos deve ser transparente e seguir todas as normas de controle, visando a economia e a responsabilidade fiscal.\n",
        "\n",
        "O planejamento eficiente dos recursos logísticos é um dos pilares para a manutenção da prontidão e da capacidade operacional das Forças Armadas. Entre os diversos insumos estratégicos, a alimentação das organizações militares desempenha papel central, tanto no aspecto orçamentário quanto no suporte direto às atividades diárias. Na Marinha do Brasil, a gestão dos estoques e dos gastos com gêneros alimentícios envolve múltiplos órgãos e abrange um volume expressivo de transações financeiras e contábeis, tornando-se um processo complexo e suscetível a variações sazonais, econômicas e administrativas.\n",
        "\n",
        "Neste cenário, prever com maior precisão os custos relacionados ao consumo de alimentos é fundamental para otimizar a alocação de recursos públicos, reduzir desperdícios, evitar rupturas de estoque e aumentar a eficiência do planejamento orçamentário. Tradicionalmente, esse processo é conduzido por meio de análises históricas e técnicas de planejamento administrativo. No entanto, tais abordagens muitas vezes não capturam adequadamente os padrões temporais e as variáveis externas que influenciam os gastos.\n",
        "\n",
        "A ciência de dados, e em particular as técnicas de modelagem de séries temporais, surge como uma alternativa poderosa para aprimorar esse processo decisório. Modelos como SARIMA, Prophet, XGBoost e LSTM permitem identificar tendências, sazonalidades e anomalias nos dados, possibilitando não apenas previsões mais robustas, mas também a geração de insights que subsidiam políticas de abastecimento e aquisição.\n",
        "\n",
        "Assim, o presente trabalho propõe a aplicação de técnicas de análise e previsão de séries temporais sobre os dados históricos de consumo de alimentos da Marinha do Brasil, com o objetivo de estimar os custos futuros e explorar padrões relevantes que possam apoiar o processo de gestão logística e orçamentária. A relevância deste estudo reside não apenas no ganho potencial de eficiência administrativa, mas também na contribuição para a transparência, a racionalização do gasto público e a modernização da gestão de suprimentos em instituições estratégicas para o país."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EDC39fUr3nJB",
      "metadata": {
        "id": "EDC39fUr3nJB"
      },
      "source": [
        "## Modelagem"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N_AvpbxG3uIJ",
      "metadata": {
        "id": "N_AvpbxG3uIJ"
      },
      "source": [
        "<div align=\"justify\">\n",
        "\n",
        " O conjunto de dados que será apresentado traz informações sobre despesas mensais com alimentação de militares e servidores de grandes organizações.\n",
        "\n",
        " O balanço de paiol do mês anterior nos traz a informação valor total dos gêneros alimentícios armazenados na organização no último dia do mês anterior. (paiol é a instalação física destinada ao estoque dos alimentos)\n",
        "\n",
        " Os gêneros podem ser adquiridos pelas organizações de quatro formas diferentes:\n",
        " - adquirindo os gêneros dos depósitos de subsistência da Marinha\n",
        " - adquirindo os gêneros através de listas de fornecimento de gêneros, que são licitações centralizadas realizadas para atender toda a Marinha\n",
        " - adquirindo os gêneros através da realização de licitações próprias\n",
        " - adquirindo os gêneros através de contratação direta, sem licitação\n",
        "\n",
        " As organizações podem transferir gêneros entre seus estoques, através da realização de remessas. Os gêneros são contabilizados então no paiol através de remessas recebidas e remesas expedidas.\n",
        "\n",
        " Os gêneros consumidos durante as refeições do dia (café da manhã, almoço, janta e ceia) são contabilizados como gêneros consumidos.\n",
        "\n",
        " Os gêneros consumidos fora das refeições, como o biscoito, café e açúcar que são consumidos durante o dia, são contabilizados como vales-extra.\n",
        "\n",
        " As eventuais perdas de estoque são contabilizadas como termos de despesa.\n",
        "\n",
        " Quanto às receitas, cada comensal lotado na organização autoriza um determinado valor despesa por dia. A soma dessa despesa autorizada no mês é o valor limite dos gêneros que poderão ser retirados do paiol.\n",
        "\n",
        " A modelagem para os dispêndios com gêneros alimentícios considera as seguintes variáveis:\n",
        " - a quantidade de pessoas às quais é oferecida alimentação\n",
        " - o custo dos alimentos em paiol\n",
        " - a composição do cardápio (englobando o perfil de consumo de cada organização)\n",
        "\n",
        " Assim, o gasto mensal $Y_m$ de determinada organização no mês $m$ pode ser expresso em termos de:\n",
        " - Efetivo atendido ($N_m$)\n",
        " - Custo de aquisição dos insumos ($P_m$)\n",
        " - Composição do cardápio e perfil de consumo ($C_m$)\n",
        "\n",
        " Ou seja, $Y_m = f(N_m, P_m, C_m)\\ +\\ ϵ_m$\n",
        "\n",
        " sendo que o gasto mensal $Y_m$ é o somatório dos gastos diários $Y_d$, expressos por:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{Y_d} = \\sum_{i=1}^q \\ N_i \\cdot p_i\\\\ \\mathbf{Y_m} = \\sum_{i=1}^{30} Y_{di}\n",
        "\\end{align}\n",
        "\n",
        "A equação apresentada mostra que o gasto mensal é resultado da interação entre o número de comensais atendidos, os preços dos gêneros alimentícios e a composição do cardápio de cada organização. O termo de erro $ϵ_m$ representa variações não explicadas diretamente pelas variáveis observadas, como oscilações sazonais de consumo, eventos extraordinários (como cerimônias ou exercícios operativos) ou perdas não previstas.\n",
        "\n",
        "Dessa forma, o problema de previsão de despesas com gêneros alimentícios pode ser tratado como um problema de série temporal multivariada, em que o objetivo é estimar $Y_m$ com base no histórico de consumo e nas variáveis explicativas ($N_m$, $P_m$, $C_m$). Essa abordagem permite capturar tanto os efeitos de tendência (crescimento ou redução do efetivo ao longo do tempo), quanto os efeitos sazonais (variações periódicas relacionadas ao calendário ou a práticas alimentares específicas), além de fatores idiossincráticos de cada organização.\n",
        "\n",
        "A correta modelagem desse problema fornece subsídios para o planejamento orçamentário, contribuindo para uma alocação mais eficiente dos recursos destinados ao municiamento, evitando tanto faltas quanto execessos que comprometam a gestão financeira e logística da alimentação dos militares e servidores.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3sDv8fXfJeYN",
      "metadata": {
        "id": "3sDv8fXfJeYN"
      },
      "source": [
        "## Trabalho"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IcDCsWxvPIxI",
      "metadata": {
        "id": "IcDCsWxvPIxI",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Downloads necessários\n",
        "\n",
        "# SE é a primeira vez que esta célula está sendo executada na sessão ENTÃO baixe os arquivos hospedados no github E instale as dependências do projeto.\n",
        "![ ! -f '/content/pip_log.txt' ] && git clone 'https://github.com/ffserro/MVP.git' && pip uninstall torchvision torch torchaudio -y > '/content/pip_log.txt' && pip install -r '/content/MVP/requirements.txt' > '/content/pip_log.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acb69f76",
      "metadata": {
        "id": "acb69f76",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Import de bibliotecas\n",
        "\n",
        "# ============================================================\n",
        "# Utilitários do sistema e manipulação de arquivos/datas\n",
        "# ============================================================\n",
        "\n",
        "from glob import glob                                         # Seleção de múltiplos arquivos\n",
        "from datetime import datetime as dt, timedelta as td          # Manipulação de datas e intervalos de tempo\n",
        "import matplotlib.dates as mdates                             # Manipulação de datas dos eixos dos gráficos\n",
        "import itertools                                              # Criação de combinações e iterações eficientes\n",
        "\n",
        "# ============================================================\n",
        "# Manipulação numérica e de dados\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd                                           # Estruturas de dados tabulares (DataFrames)\n",
        "import numpy as np                                            # Computação numérica de alta performance (arrays/vetores)\n",
        "import requests                                               # Requisições HTTP para buscar dados na grande rede mundial de computadores\n",
        "\n",
        "# ============================================================\n",
        "# Visualização de dados\n",
        "# ============================================================\n",
        "\n",
        "import matplotlib.pyplot as plt                               # Visualizações estáticas básicas (gráficos 2D)\n",
        "import seaborn as sns                                         # Visualizações estatísticas de alto nível (heatmaps, distribuições)\n",
        "import plotly.express as px                                   # Visualizações interativas de alto nível\n",
        "import plotly.graph_objects as go                             # Visualizações interativas detalhadas e customizáveis\n",
        "\n",
        "# ============================================================\n",
        "# Ferramentas estatísticas para séries temporais\n",
        "# ============================================================\n",
        "\n",
        "import statsmodels.api as sm                                  # Modelos estatísticos gerais\n",
        "from statsmodels.tsa.stattools import adfuller                # Teste ADF (estacionariedade)\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose       # Decomposição de série (tendência/sazonalidade)\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf # Autocorrelação e autocorrelação parcial\n",
        "\n",
        "# ============================================================\n",
        "# Modelos clássicos de séries temporais (baseline)\n",
        "# ============================================================\n",
        "\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX        # Modelo SARIMA/SARIMAX\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing  # Suavização exponencial de Holt-Winters\n",
        "\n",
        "# ============================================================\n",
        "# Preparação e avaliação de dados\n",
        "# ============================================================\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit           # Validação cruzada para séries temporais\n",
        "from sklearn.preprocessing import MinMaxScaler                # Normalização (0–1) para redes neurais e ML\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, root_mean_squared_error  # Métricas de avaliação (MAE, R2, RMSE)\n",
        "\n",
        "# ============================================================\n",
        "# Modelos modernos de previsão\n",
        "# ============================================================\n",
        "\n",
        "from prophet import Prophet                                   # Modelo Prophet (captura tendência + sazonalidade)\n",
        "from xgboost import XGBRegressor                              # Modelo baseado em boosting (árvores de decisão)\n",
        "\n",
        "# Redes neurais (TensorFlow/Keras)\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)                                        # para reprodutibilidade dos resultados, sem esquecer a toalha\n",
        "\n",
        "from tensorflow.keras.models import Sequential                # Estrutura sequencial de camadas\n",
        "from tensorflow.keras.layers import LSTM, Dense, BatchNormalization, Dropout # Camadas para deep learning\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau # Callbacks para treinamento robusto\n",
        "\n",
        "# Modelos baseados em deep learning específicos de séries temporais\n",
        "from neuralforecast import NeuralForecast\n",
        "from neuralforecast.models import NBEATS                      # Arquitetura N-BEATS (estado da arte em forecasting)\n",
        "\n",
        "# AutoML para séries temporais\n",
        "from autogluon.timeseries import TimeSeriesPredictor          # AutoGluon (seleção automática de modelos)\n",
        "\n",
        "# ============================================================\n",
        "# Configurações gerais\n",
        "# ============================================================\n",
        "\n",
        "from IPython.display import clear_output                      # para limpar as células que contém barras de progresso que estavam gerando conflito com o nbformat do github\n",
        "\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')                                      # Silencia warnings para manter a saída do notebook limpa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5b106c1",
      "metadata": {
        "id": "c5b106c1",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Leitura dos dados brutos\n",
        "\n",
        "# ============================================================\n",
        "# Leitura e consolidação dos dados brutos\n",
        "# ============================================================\n",
        "\n",
        "mmm = pd.DataFrame()                                            # cria um DataFrame vazio para armazenar os dados\n",
        "mmm = pd.concat(                                                # carrega todos os arquivos de movimentação mensal (mmm) em um único DataFrame.\n",
        "    [mmm] + [\n",
        "        pd.read_excel(arquivo, parse_dates=[['ano', 'mes']])    # parse_dates=[['ano', 'mes']] combina as colunas 'ano' e 'mes' em um único campo datetime\n",
        "        for arquivo in glob('/content/MVP/dados/mmm/*.xlsx')    # glob encontra todos os arquivos .xlsx dentro da pasta de dados\n",
        "    ],\n",
        "    ignore_index=True\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "etapas = pd.DataFrame()\n",
        "etapas = pd.concat(                                              # carrega todos os arquivos de etapas em um único DataFrame.\n",
        "    [etapas] + [\n",
        "        pd.read_excel(arquivo, parse_dates=[['ano', 'mes']])     # estrutura e lógica de leitura iguais ao dataset acima\n",
        "        for arquivo in glob('/content/MVP/dados/etapas/*.xlsx')\n",
        "    ],\n",
        "    ignore_index=True\n",
        ")\n",
        "\n",
        "\n",
        "centralizadas = pd.read_csv('/content/MVP/dados/om_centralizada.csv') # carrega as informações das organizações militares centralizadas e suas respectivas centralizadoras\n",
        "\n",
        "\n",
        "om_info = pd.read_csv('/content/MVP/dados/om_info.csv')           # carrega informações complementares sobre as organizações militares\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D2taCARVZEwt",
      "metadata": {
        "id": "D2taCARVZEwt"
      },
      "source": [
        "### Preparação dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Un4l_KcXaoz6",
      "metadata": {
        "id": "Un4l_KcXaoz6"
      },
      "source": [
        "#### Limpeza do conjunto de dados sobre os Mapas Mensais do Municiamento de 2019 a 2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i7JCu7ZWalzw",
      "metadata": {
        "id": "i7JCu7ZWalzw"
      },
      "outputs": [],
      "source": [
        "# Existem organizações que não possuem dados para todo o período analisado\n",
        "mmm.groupby('nome').ano_mes.count().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q-s3ueiHa6Lu",
      "metadata": {
        "id": "q-s3ueiHa6Lu"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Ao todo o conjunto de dados contempla 80 meses. Como será realizada uma análise de série temporal, vou considerar apenas as organizações que possuem dados para todo o período.\n",
        "mmm = mmm[mmm.codigo.isin(mmm.codigo.value_counts()[mmm.codigo.value_counts() == 80].index)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4_xJLDpda_bB",
      "metadata": {
        "id": "4_xJLDpda_bB"
      },
      "outputs": [],
      "source": [
        "mmm = mmm[mmm.ano_mes < dt(2025, 7, 1)] # removendo os dados dos últimos meses. Não são confiáveis porque as comprovações ainda não tinham sido completamente consolidadas a época da coleta de dados."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WMzcvFijbCoc",
      "metadata": {
        "id": "WMzcvFijbCoc"
      },
      "source": [
        "#### Limpeza do conjunto de dados sobre informações das OM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J0gpOj7abIzH",
      "metadata": {
        "id": "J0gpOj7abIzH"
      },
      "outputs": [],
      "source": [
        "# Verificando a existência de dados faltosos\n",
        "(om_info.isna().sum()/len(om_info)).sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WEaLd60hbRP_",
      "metadata": {
        "id": "WEaLd60hbRP_"
      },
      "outputs": [],
      "source": [
        "# Quantidade de entradas diferentes para cada coluna no conjunto de dados\n",
        "om_info.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_ZGPVcopbPZg",
      "metadata": {
        "id": "_ZGPVcopbPZg"
      },
      "outputs": [],
      "source": [
        "# Informações que não vao agregar conhecimento para o caso em tela, por serem nulos ou por conter informações irrelevantes\n",
        "om_info.drop(columns=['COMIMSUP', 'CNPJ', 'TELEFONE', 'ODS', 'TIPO_CONEXAO', 'CRIACAO', 'MODIFICACAO'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MsnEeL4abTYP",
      "metadata": {
        "id": "MsnEeL4abTYP"
      },
      "outputs": [],
      "source": [
        "om_info[['DN_ID', 'SUB_DN_ID', 'AREA_ID', 'COD_SQ_LOCAL', 'NOME', 'CIDADE']].sort_values(by=['DN_ID', 'SUB_DN_ID', 'AREA_ID', 'COD_SQ_LOCAL'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Yg0iX83rbU5h",
      "metadata": {
        "id": "Yg0iX83rbU5h"
      },
      "outputs": [],
      "source": [
        "# As colunas SUB_DN_ID, AREA_ID e COD_SQ_LOCAL são pouco ou não descritivas\n",
        "om_info.drop(columns=['SUB_DN_ID', 'AREA_ID', 'COD_SQ_LOCAL'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iItk2qWRbV-N",
      "metadata": {
        "id": "iItk2qWRbV-N"
      },
      "outputs": [],
      "source": [
        "om_info.sort_values(by='TIPO')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ibe9IhfVbX00",
      "metadata": {
        "id": "Ibe9IhfVbX00"
      },
      "outputs": [],
      "source": [
        "# A variável TIPO começa descrevendo os tipo de organizações, como A para bases aeronavais, B para bases, F para fuzileiros navais, N para navios, S para saúde e I para instrução. Porém o T entra em uma categoria geral como se em algum momento essa vaiável deixou de ser utilizada.\n",
        "# Então se tornou pouco descritiva para os nossos objetivos\n",
        "om_info.drop(columns=['TIPO'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qFC_VhAGbY9P",
      "metadata": {
        "id": "qFC_VhAGbY9P"
      },
      "outputs": [],
      "source": [
        "# observações com dados faltosos\n",
        "om_info.loc[om_info.isna().sum(axis=1) != 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qDD21OiibaRy",
      "metadata": {
        "id": "qDD21OiibaRy"
      },
      "outputs": [],
      "source": [
        "# Preenchendo manualmente os dados faltosos com informações da internet\n",
        "om_info.loc[om_info.CODIGO==87310, 'BAIRRO'] = 'Plano Diretor Sul'\n",
        "om_info.loc[om_info.CODIGO==87700, 'BAIRRO'] = 'Asa Sul'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IBy35wKwbcIT",
      "metadata": {
        "id": "IBy35wKwbcIT"
      },
      "source": [
        "#### Limpeza de dados do conjunto de dados sobre centralização do municiamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HBIVBMedbiqR",
      "metadata": {
        "id": "HBIVBMedbiqR"
      },
      "outputs": [],
      "source": [
        "# Primeira transformação a ser feita será padronizar a codificação das organizações por UASG\n",
        "centralizadas['OM_CENTRALIZADA_ID'] = centralizadas.OM_CENTRALIZADA_ID.map(om_info.set_index('ID').CODIGO)\n",
        "centralizadas['OM_CENTRALIZADORA_ID'] = centralizadas.OM_CENTRALIZADORA_ID.map(om_info.set_index('ID').CODIGO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R6UyXDrDbmGD",
      "metadata": {
        "id": "R6UyXDrDbmGD"
      },
      "outputs": [],
      "source": [
        "# Mais uma vez, eu só preciso das informações das organizações que estão presentes no conjunto de dados do Mapa Mensal do Municiamento\n",
        "centralizadas = centralizadas[centralizadas.OM_CENTRALIZADORA_ID.isin(mmm.codigo.unique())]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xCN5KQq8bnaJ",
      "metadata": {
        "id": "xCN5KQq8bnaJ"
      },
      "outputs": [],
      "source": [
        "# Drop de colunas pouco informativas para o problema em tela\n",
        "centralizadas.drop(columns=['CONTATO', 'TELEFONE', 'CRIACAO', 'MODIFICACAO', 'GESTORIA_ID'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P4YlPXPzbowk",
      "metadata": {
        "id": "P4YlPXPzbowk"
      },
      "outputs": [],
      "source": [
        "# Remover do conjunto de dados as movimentações que aconteceram antes do período observado\n",
        "centralizadas = centralizadas[~(pd.to_datetime(centralizadas.DATA_FIM) < dt(2019, 1, 1))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LMybqLoNbqkc",
      "metadata": {
        "id": "LMybqLoNbqkc"
      },
      "outputs": [],
      "source": [
        "# Verificação manual da coerência dos períodos municiados\n",
        "centralizadas.groupby('OM_CENTRALIZADA_ID').filter(lambda x: len(x)> 1).sort_values(by=['OM_CENTRALIZADA_ID', 'DATA_INICIO'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XKRjKiMCbuBP",
      "metadata": {
        "id": "XKRjKiMCbuBP"
      },
      "outputs": [],
      "source": [
        "# Definindo algumas datas que as organizações passaram a ser centralizadas por outra centralizadora\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==11500) & (centralizadas.OM_CENTRALIZADORA_ID==71000), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==11500) & (centralizadas.OM_CENTRALIZADORA_ID==81000), 'DATA_INICIO']\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==49000) & (centralizadas.OM_CENTRALIZADORA_ID==71000), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==49000) & (centralizadas.OM_CENTRALIZADORA_ID==81000), 'DATA_INICIO']\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==62000) & (centralizadas.OM_CENTRALIZADORA_ID==62000), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==62000) & (centralizadas.OM_CENTRALIZADORA_ID==81000), 'DATA_INICIO']\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==62500) & (centralizadas.OM_CENTRALIZADORA_ID==62500), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==62500) & (centralizadas.OM_CENTRALIZADORA_ID==81000), 'DATA_INICIO']\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==64000) & (centralizadas.OM_CENTRALIZADORA_ID==64000), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==64000) & (centralizadas.OM_CENTRALIZADORA_ID==81000), 'DATA_INICIO']\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==65701) & (centralizadas.OM_CENTRALIZADORA_ID==65701), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==65701) & (centralizadas.OM_CENTRALIZADORA_ID==81000), 'DATA_INICIO']\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==65730) & (centralizadas.OM_CENTRALIZADORA_ID==65701), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==65730) & (centralizadas.OM_CENTRALIZADORA_ID==81000), 'DATA_INICIO']\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==67000) & (centralizadas.OM_CENTRALIZADORA_ID==62000), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==67000) & (centralizadas.OM_CENTRALIZADORA_ID==81000), 'DATA_INICIO']\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==71000) & (centralizadas.OM_CENTRALIZADORA_ID==71000), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==71000) & (centralizadas.OM_CENTRALIZADORA_ID==71100), 'DATA_INICIO']\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==71000) & (centralizadas.OM_CENTRALIZADORA_ID==71100), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==71000) & (centralizadas.OM_CENTRALIZADORA_ID==81000), 'DATA_INICIO']\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==72000) & (centralizadas.OM_CENTRALIZADORA_ID==71000), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==72000) & (centralizadas.OM_CENTRALIZADORA_ID==81000), 'DATA_INICIO']\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==73000) & (centralizadas.OM_CENTRALIZADORA_ID==71000), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==73000) & (centralizadas.OM_CENTRALIZADORA_ID==81000), 'DATA_INICIO']\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==73200) & (centralizadas.OM_CENTRALIZADORA_ID==71000), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==73200) & (centralizadas.OM_CENTRALIZADORA_ID==81000), 'DATA_INICIO']\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==76000) & (centralizadas.OM_CENTRALIZADORA_ID==71000), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==76000) & (centralizadas.OM_CENTRALIZADORA_ID==81000), 'DATA_INICIO']\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==78000) & (centralizadas.OM_CENTRALIZADORA_ID==81000), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==78000) & (centralizadas.OM_CENTRALIZADORA_ID==71000), 'DATA_INICIO']\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==80000) & (centralizadas.OM_CENTRALIZADORA_ID==80000), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==80000) & (centralizadas.OM_CENTRALIZADORA_ID==81000), 'DATA_INICIO']\n",
        "\n",
        "# Removendo algumas informações que estavam duplicadas\n",
        "centralizadas.drop(index=centralizadas[(centralizadas.OM_CENTRALIZADA_ID==62600) & (centralizadas.OM_CENTRALIZADORA_ID==62600) & (centralizadas.TIPO_CENTRALIZACAO_ID.isna())].index, inplace=True)\n",
        "centralizadas.drop(index=centralizadas[(centralizadas.OM_CENTRALIZADA_ID==87400) & (centralizadas.OM_CENTRALIZADORA_ID==87400) & (centralizadas.TIPO_CENTRALIZACAO_ID.isna())].index, inplace=True)\n",
        "centralizadas.drop(index=centralizadas[(centralizadas.OM_CENTRALIZADA_ID==88000) & (centralizadas.OM_CENTRALIZADORA_ID==88000) & (centralizadas.TIPO_CENTRALIZACAO_ID.isna())].index, inplace=True)\n",
        "centralizadas.drop(index=centralizadas[(centralizadas.OM_CENTRALIZADA_ID==88133) & (centralizadas.OM_CENTRALIZADORA_ID==88133) & (centralizadas.TIPO_CENTRALIZACAO_ID.isna())].index, inplace=True)\n",
        "centralizadas.drop(index=centralizadas[(centralizadas.OM_CENTRALIZADA_ID==88701) & (centralizadas.OM_CENTRALIZADORA_ID==88000) & (centralizadas.DATA_FIM.isna())].index, inplace=True)\n",
        "centralizadas.drop(index=centralizadas[(centralizadas.OM_CENTRALIZADA_ID==95300) & (centralizadas.OM_CENTRALIZADORA_ID==95380) & (centralizadas.TIPO_CENTRALIZACAO_ID.isna())].index, inplace=True)\n",
        "centralizadas.drop(index=centralizadas[(centralizadas.OM_CENTRALIZADA_ID==95340) & (centralizadas.OM_CENTRALIZADORA_ID==95380) & (centralizadas.TIPO_CENTRALIZACAO_ID.isna())].index, inplace=True)\n",
        "centralizadas.drop(index=centralizadas[(centralizadas.OM_CENTRALIZADA_ID==95370) & (centralizadas.OM_CENTRALIZADORA_ID==95380) & (centralizadas.TIPO_CENTRALIZACAO_ID.isna())].index, inplace=True)\n",
        "centralizadas.drop(index=centralizadas[(centralizadas.OM_CENTRALIZADA_ID==95380) & (centralizadas.OM_CENTRALIZADORA_ID==95380) & (centralizadas.DATA_INICIO==dt(2004, 1, 1))].index, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "klA8qECFbvRI",
      "metadata": {
        "id": "klA8qECFbvRI"
      },
      "outputs": [],
      "source": [
        "# Supondo que as relações que não possuem data fim estão em vigor até hoje\n",
        "centralizadas.DATA_FIM.fillna(dt(2026,1,1), inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4kc_m-hDbwnc",
      "metadata": {
        "id": "4kc_m-hDbwnc"
      },
      "source": [
        "#### Limpeza do conjunto de dados sobre etapas do municiamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8eTK5TjYb0xh",
      "metadata": {
        "id": "8eTK5TjYb0xh"
      },
      "outputs": [],
      "source": [
        "# Filtro para manter apenas etapas que sejam relevantes dado as organizações contantes do conjunto de dados dos Mapas Mensais do Municiamento\n",
        "etapas = etapas[etapas.uasg.isin(mmm.codigo.unique())]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nz_1qH87b23h",
      "metadata": {
        "id": "nz_1qH87b23h"
      },
      "outputs": [],
      "source": [
        "# Removendo as etapas de complementos, uma vez que o objetivo da contabilização das etapas é contar o número de pessoas de cada organização\n",
        "etapas = etapas[~(etapas.codigo_etapa//100==6)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pNsMWpnjcAfp",
      "metadata": {
        "id": "pNsMWpnjcAfp"
      },
      "source": [
        "#### Salvando os dados limpos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iaJ1PP35b_me",
      "metadata": {
        "id": "iaJ1PP35b_me"
      },
      "outputs": [],
      "source": [
        "mmm.to_csv('/content/MVP/dados/mmm/mmm_limpo.csv', index=False)\n",
        "om_info.to_csv('/content/MVP/dados/om_info_limpo.csv', index=False)\n",
        "centralizadas.to_csv('/content/MVP/dados/om_centralizada_limpo.csv', index=False)\n",
        "etapas.to_csv('/content/MVP/dados/etapas/etapas_limpo.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sbYdXOLiEqAY",
      "metadata": {
        "id": "sbYdXOLiEqAY"
      },
      "source": [
        "#### Criação dos datasets da série temporal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d23b319b",
      "metadata": {
        "id": "d23b319b"
      },
      "outputs": [],
      "source": [
        "mmm_marinha = mmm.groupby(['ano_mes'])[['totais_balanco_paiol_despesa']].sum().reset_index().rename(columns={'totais_balanco_paiol_despesa':'consumo'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81640c41",
      "metadata": {
        "id": "81640c41"
      },
      "outputs": [],
      "source": [
        "mmm_marinha = mmm_marinha.iloc[:-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jVCau_ZSeEzU",
      "metadata": {
        "id": "jVCau_ZSeEzU"
      },
      "outputs": [],
      "source": [
        "mmm_marinha.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f51d785",
      "metadata": {
        "id": "0f51d785"
      },
      "outputs": [],
      "source": [
        "mmm_etapas = pd.merge(left=mmm, right=etapas, how='inner', left_on=['ano_mes', 'codigo'], right_on=['ano_mes', 'uasg'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fe04974",
      "metadata": {
        "id": "4fe04974"
      },
      "outputs": [],
      "source": [
        "# Filtra o dataframe mmm_etapas para incluir apenas os códigos de etapa 103 e 105, que representam diferentes tipos de refeições ou etapas de municiamento.\n",
        "# Em seguida, seleciona as colunas 'ano_mes', 'nome', 'codigo_etapa' e 'quantidade' para visualização.\n",
        "mmm_etapas[mmm_etapas.codigo_etapa.isin([103, 105])][['ano_mes', 'nome', 'codigo_etapa', 'quantidade']]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "om_info = om_info[om_info.CODIGO.isin(mmm.codigo.unique())] # mantendo as informações apenas das organizações selecionadas no dataset mmm"
      ],
      "metadata": {
        "id": "NpIBbYsnKF8P"
      },
      "id": "NpIBbYsnKF8P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5HTsVvFHEco_",
      "metadata": {
        "id": "5HTsVvFHEco_"
      },
      "source": [
        "### Verificações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "066e46ec",
      "metadata": {
        "id": "066e46ec"
      },
      "outputs": [],
      "source": [
        "def plota_resultados(title, df=mmm_marinha, x_col='ano_mes', y_col='consumo', preds=None, labels=None):\n",
        "    '''\n",
        "    Plota a série temporal original e, opcionalmente, as previsões de um ou mais modelos.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame contendo a série temporal original.\n",
        "        x_col (str): Nome da coluna do eixo x (geralmente a coluna de data).\n",
        "        y_col (str): Nome da coluna do eixo y (a variável a ser plotada).\n",
        "        title (str): Título do gráfico.\n",
        "        preds (dict, optional): Dicionário onde as chaves são os nomes dos modelos\n",
        "                                 e os valores são tuplas (x_pred, y_pred) com as\n",
        "                                 datas e os valores previstos. Defaults to None.\n",
        "        labels (dict, optional): Dicionário para renomear os rótulos dos eixos\n",
        "                                 no gráfico. Defaults to {x_col: 'Período', y_col: 'Valor observado'}.\n",
        "    '''\n",
        "    # Série real\n",
        "    fig = px.line(\n",
        "        df,\n",
        "        x=x_col,\n",
        "        y=y_col,\n",
        "        labels=labels or {x_col: 'Período', y_col: 'Valor observado'},\n",
        "        title=title\n",
        "    )\n",
        "\n",
        "    # Previsões opcionais\n",
        "    if preds:\n",
        "        for model_name, (y_pred) in preds.items():\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=df[x_col].iloc[-12:],\n",
        "                y=y_pred,\n",
        "                mode='lines+markers',\n",
        "                name=f'Previsão — {model_name}',\n",
        "                line=dict(width=2, dash='dash')\n",
        "            ))\n",
        "\n",
        "    # Layout padronizado\n",
        "    fig.update_traces(line=dict(width=2))\n",
        "    fig.update_xaxes(tickangle=45)\n",
        "    fig.update_layout(\n",
        "        template='plotly_white',\n",
        "        hovermode='x unified'\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "# Gera o plot da série temporal original dos gastos com alimentação\n",
        "fig = plota_resultados(\n",
        "title='Gastos com alimentação dos últimos cinco anos'\n",
        ")\n",
        "\n",
        "# Exibe o gráfico\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc3e4dff",
      "metadata": {
        "id": "fc3e4dff"
      },
      "outputs": [],
      "source": [
        "def checa_estacionariedade(serie, alpha=0.05):\n",
        "    '''\n",
        "    Executa o Teste de Dickey-Fuller Aumentado (ADF) para verificar\n",
        "    se uma série temporal é estacionária.\n",
        "\n",
        "    Parâmetros\n",
        "    ----------\n",
        "    serie : pd.Series\n",
        "        Série temporal a ser testada.\n",
        "    alpha : float, default=0.05\n",
        "        Nível de significância para o teste de hipótese.\n",
        "\n",
        "    Interpretação\n",
        "    -------------\n",
        "    H0 (hipótese nula): a série possui raiz unitária (não estacionária).\n",
        "    H1 (alternativa): a série é estacionária.\n",
        "    '''\n",
        "\n",
        "    resultado = adfuller(serie.dropna())                        # remove valores nulos antes do teste\n",
        "\n",
        "    estatistica_adf = resultado[0]                              # extrai estatísticas relevantes\n",
        "    p_valor = resultado[1]\n",
        "    num_lags = resultado[2]                                     # número de lags usados no modelo\n",
        "    num_obs = resultado[3]                                      # número de observações utilizadas\n",
        "\n",
        "    print(f'Estatística ADF: {estatistica_adf:.4f}')            # exibição dos principais resultados\n",
        "    print(f'p-valor: {p_valor:.4f}')\n",
        "    print(f'Nº lags utilizados: {num_lags}')\n",
        "    print(f'Nº observações: {num_obs}')\n",
        "    print('Valores críticos:', resultado[4])                    # níveis de significância (1%, 5%, 10%)\n",
        "\n",
        "\n",
        "    if p_valor < alpha:                                         # avaliação da hipótese nula\n",
        "        print('Série estacionária (rejeita H0 de raiz unitária).')\n",
        "    else:\n",
        "        print('Série não estacionária (não rejeita H0).')\n",
        "\n",
        "# ============================================================\n",
        "# Verificação da estacionariedade da série de consumo\n",
        "# ============================================================\n",
        "\n",
        "checa_estacionariedade(mmm_marinha['consumo'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"justify\">\n",
        "\n",
        "Uma série temporal é considerada não estacionária quando suas propriedades estatísticas, como média, variância e covariância, mudam ao longo do tempo. Isso significa que a série apresenta tendência, sazonalidade ou mudanças estruturais que tornam seu comportamento instável e imprevisível se analisado diretamente. Em outras palavras, os padrões da série não são constantes, o que dificulta a modelagem e a previsão sem antes aplicar técnicas de transformação, como diferenciação ou remoção de tendência.\n",
        "\n",
        "<hr>"
      ],
      "metadata": {
        "id": "Jr_JHKi2V4si"
      },
      "id": "Jr_JHKi2V4si"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mQPb3Dg5Ev5D",
      "metadata": {
        "id": "mQPb3Dg5Ev5D"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Decomposição da série temporal em tendência, sazonalidade e resíduos\n",
        "# ============================================================\n",
        "\n",
        "decomp = seasonal_decompose(\n",
        "    mmm_marinha.set_index('ano_mes')['consumo'],                # consumo = tendência + sazonalidade + resíduo\n",
        "    model='additive',                                           # aplica decomposição aditiva\n",
        "    period=12                                                   # período de sazonalidade anual assume a mensalidade em 12 meses.\n",
        ")\n",
        "\n",
        "# Gera visualização dos componentes:\n",
        "# Série observada\n",
        "# Tendência (variação de longo prazo)\n",
        "# Sazonalidade (padrões que se repetem periodicamente)\n",
        "# Resíduos (parte não explicada pelo modelo)\n",
        "fig = decomp.plot()\n",
        "fig.set_size_inches(20, 8)\n",
        "\n",
        "for ax in fig.axes:\n",
        "    ax.set_xlabel(\"Meses\")\n",
        "    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
        "    ax.xaxis.set_major_formatter(\n",
        "        plt.matplotlib.dates.DateFormatter(\"%b/%Y\")             # formato \"Mes/Ano\"\n",
        "    )\n",
        "    ax.tick_params(axis=\"x\", rotation=45)\n",
        "\n",
        "plt.suptitle(\"Decomposição da Série Temporal (Modelo Aditivo)\", fontsize=14, y=1.02)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"justify\">\n",
        "\n",
        "A decomposição de uma série temporal consiste em separar seus componentes fundamentais para melhor compreensão e análise do comportamento dos dados. Em geral, a série pode ser dividida em três partes: tendência, que representa a direção de longo prazo (se os valores estão crescendo, diminuindo ou se mantendo estáveis ao longo do tempo); sazonalidade, que corresponde a padrões que se repetem em intervalos regulares, como meses ou estações do ano; e resíduos (ou ruído), que englobam as variações aleatórias não explicadas pelos dois primeiros componentes. Essa separação é útil porque permite identificar padrões ocultos, avaliar a previsibilidade da série e escolher modelos de previsão mais adequados."
      ],
      "metadata": {
        "id": "jLlAHe6XWvmn"
      },
      "id": "jLlAHe6XWvmn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"justify\">\n",
        "\n",
        "Como no caso em tela estamos tratando do preço de alimentos, existe uma variável exógena que pode ser suficientemente descritiva, que é o Índice Nacional de Preços ao Consumidor Amplo (IPCA), que abrange em seu cálculo os custos de alimentação do país."
      ],
      "metadata": {
        "id": "QA3n7WLbAKBP"
      },
      "id": "QA3n7WLbAKBP"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Aquisição da série histórica do IPCA para comparação com a tendência\n",
        "# ============================================================\n",
        "\n",
        "ipca = requests.get('https://www.dadosdemercado.com.br/indices/ipca') # request para aquisição do html de um site disponível ao público com índices históricos do IPCA\n",
        "ipca = str(ipca.content)[str(ipca.content).find('const data'):str(ipca.content).find(';', str(ipca.content).find('const data'))].strip('const data = ') # filtragem dos dados de ipca\n",
        "ipca = pd.DataFrame(eval(ipca), columns=['ano_mes', 'variacao_ipca']).set_index('ano_mes') # os mesmos dados, mas em um dataframe que podemos manipular\n",
        "ipca.index = pd.to_datetime(ipca.index)                         # para que os índices sejam tratados como datas\n",
        "ipca = ipca.loc[decomp.trend.dropna().index] * 1e-2             # porque na verdade os índices são porcentagens"
      ],
      "metadata": {
        "id": "e7dMjyAv8UPc"
      },
      "id": "e7dMjyAv8UPc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tendencia_consumo = decomp.trend.dropna().copy()                # tendência obtida após a decomposição da série temporal\n",
        "inflacao_ipca = tendencia_consumo.copy()                        # dataset que ainda vai ser populado com dados hipotéticos, caso a tedência dependesse apenas do IPCA"
      ],
      "metadata": {
        "id": "Za1c8LXV9TBv"
      },
      "id": "Za1c8LXV9TBv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for mes in inflacao_ipca.index[1:]:\n",
        "  inflacao_ipca.loc[mes] = inflacao_ipca.shift(1).loc[mes] * (ipca.loc[mes, 'variacao_ipca'] + 1) # aplica-se a variação de juros sucessivamente desde a primeira observação dos dados de tendência"
      ],
      "metadata": {
        "id": "jVyrFbQD9Y-f"
      },
      "id": "jVyrFbQD9Y-f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20, 8))\n",
        "plt.plot(tendencia_consumo)\n",
        "plt.plot(inflacao_ipca - abs(tendencia_consumo - inflacao_ipca).mean())\n",
        "plt.show()                                                      #sobreposição dos gráficos de tendência e variação da inflação"
      ],
      "metadata": {
        "id": "JZU9kNtO97hC"
      },
      "id": "JZU9kNtO97hC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2_score(tendencia_consumo, inflacao_ipca - abs(tendencia_consumo - inflacao_ipca).mean()) # quanto o IPCA explica a tendência observada?"
      ],
      "metadata": {
        "id": "OT_N15tm_U9v"
      },
      "id": "OT_N15tm_U9v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<div align=\"justify\">\n",
        "\n",
        "Parece que o IPCA explica a tendência suficientemente bem, levando em consideração que o IPCA também abrange outras variações de preço como habitação e transportes que também são muito voláteis, e que do decorrer do período analisado a pandemia teve grande influência sobre a variação de preços também.\n",
        "Além disso, o IPCA é uma média nacional de variação. As organizações analisadas se encontram em vários estados diferentes, sendo que esta distribuição não é propriamente representativa dos estados, por não ser balanceada nesse sentido.\n",
        "<hr>"
      ],
      "metadata": {
        "id": "pIH_NvpID1OA"
      },
      "id": "pIH_NvpID1OA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"justify\">\n",
        "\n",
        "A análise de autocorrelação (ACF) e autocorrelação parcial (PACF) são ferramentas fundamentais no estudo de séries temporais, pois ajudam a entender como os valores passados influenciam os valores presentes da série.\n",
        "A autocorrelação mede a correlação entre a série e suas defasagens (lags), ou seja, verifica se há relação entre um valor e os valores anteriores em diferentes intervalos de tempo. Já a autocorrelação parcial busca identificar essa relação de forma mais “direta”, isolando o efeito das defasagens intermediárias e mostrando a correlação entre um valor e um lag específico sem a influência de outros. Essas análises são especialmente úteis para identificar padrões sazonais e para auxiliar na escolha dos parâmetros de modelos como o ARIMA, onde o ACF e o PACF orientam a definição de termos autorregressivos (AR) e de médias móveis (MA)."
      ],
      "metadata": {
        "id": "NX9xLv6QXKWY"
      },
      "id": "NX9xLv6QXKWY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gUJDBObHEzh1",
      "metadata": {
        "id": "gUJDBObHEzh1"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Análise de Autocorrelação (ACF) e Autocorrelação Parcial (PACF)\n",
        "# ============================================================\n",
        "\n",
        "# Cria figura com dois subplots: um para ACF e outro para PACF\n",
        "fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
        "\n",
        "# Autocorrelação (ACF)\n",
        "# - Mede a correlação da série com suas próprias defasagens\n",
        "# - Útil para identificar dependência temporal e possíveis lags para ARIMA\n",
        "plot_acf(\n",
        "    mmm_marinha['consumo'].dropna(),  # remove NaNs antes do cálculo\n",
        "    lags=36,                           # número de defasagens a serem exibidas\n",
        "    ax=ax[0]\n",
        ")\n",
        "ax[0].set_title(\"Autocorrelação (ACF) - Consumo\")\n",
        "\n",
        "# Autocorrelação Parcial (PACF)\n",
        "# - Mede a correlação da série com uma defasagem específica,\n",
        "#   removendo efeitos das defasagens intermediárias\n",
        "# - Ajuda a identificar a ordem AR (p) em modelos ARIMA\n",
        "plot_pacf(\n",
        "    mmm_marinha['consumo'].dropna(),\n",
        "    lags=36,\n",
        "    ax=ax[1],\n",
        "    method='ywm'   # método de estimação robusto para PACF\n",
        ")\n",
        "ax[1].set_title(\"Autocorrelação Parcial (PACF) - Consumo\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"justify\">\n",
        "\n",
        "Na análise de autocorrelação, os primeiros lags apresentam autocorrelação positiva e significativa, acima do intervalo de confiança, significando que os valores da série estão fortemente relacionados com os valores passados próximos.\n",
        "O decaimento gradual da autocorrelação confirma a presença de um forte componente de tendência.\n",
        "\n",
        "Na análise da autocorrelação parcial, o lag 1 é bastante significativo e explica a maior parte da dependência. Há outros picos significativos (lags 5 e 13), mas d emenor intensidade, indicando que pode existir ação de sazonalidade."
      ],
      "metadata": {
        "id": "AXMQkwsfYyR1"
      },
      "id": "AXMQkwsfYyR1"
    },
    {
      "cell_type": "markdown",
      "id": "0mYyWRosQH0C",
      "metadata": {
        "id": "0mYyWRosQH0C"
      },
      "source": [
        "### Teste de modelos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = mmm_marinha.iloc[:-12]\n",
        "test = mmm_marinha.iloc[-12:]"
      ],
      "metadata": {
        "id": "d8dtYUKvigHi"
      },
      "id": "d8dtYUKvigHi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "-07Wb9xwSs4S",
      "metadata": {
        "id": "-07Wb9xwSs4S"
      },
      "source": [
        "#### Previsão Naïve (critério de comparação)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"justify\">\n",
        "\n",
        "A previsão naive em séries temporais é um método simples que assume que o valor futuro será igual ao último valor observado (ou ao valor correspondente de um período anterior, no caso da versão sazonal). Apesar de sua simplicidade, ela é importante porque serve como modelo de referência (baseline): qualquer modelo mais sofisticado deve apresentar desempenho melhor do que a previsão naive para justificar sua complexidade. Assim, ela será usada aqui para estabelecer um ponto de comparação na avaliação da qualidade das previsões."
      ],
      "metadata": {
        "id": "uBvgPl30j7Hu"
      },
      "id": "uBvgPl30j7Hu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80c6cb01",
      "metadata": {
        "id": "80c6cb01"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Previsão Naïve (Baseline)\n",
        "# ============================================================\n",
        "\n",
        "# A previsão \"naïve\" assume que o valor futuro é igual ao último valor observado.\n",
        "# Aqui usamos shift(1) para criar previsões deslocadas em 1 período.\n",
        "previsao_naive = mmm_marinha['consumo'].shift(1)\n",
        "\n",
        "y_pred_naive = previsao_naive[-12:]\n",
        "\n",
        "mae_naive = mean_absolute_error(mmm_marinha.consumo.iloc[-12:], y_pred_naive)                # erro médio absoluto\n",
        "rmse_naive = root_mean_squared_error(mmm_marinha.consumo.iloc[-12:], y_pred_naive)           # raiz do erro quadrático médio\n",
        "r2_naive = r2_score(mmm_marinha.consumo.iloc[-12:], y_pred_naive)                            # coeficiente de determinação\n",
        "\n",
        "print(f\"Previsão Naïve -> MAE: {mae_naive:.2f} | RMSE: {rmse_naive:.2f} | R2: {r2_naive}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plota_resultados(\n",
        "    title=\"Previsão temporal — Naïve\",\n",
        "    preds={\n",
        "        \"Naïve\": (y_pred_naive)\n",
        "    }\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "yG6HgOAnylG7"
      },
      "id": "yG6HgOAnylG7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6vQyvOIWS05M",
      "metadata": {
        "id": "6vQyvOIWS05M"
      },
      "source": [
        "#### Previsão SARIMA (critério de comparação)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"justify\">\n",
        "\n",
        "A previsão SARIMA (Seasonal AutoRegressive Integrated Moving Average) é uma técnica de séries temporais que combina componentes autorregressivos, de média móvel e de diferenciação para capturar tanto padrões de tendência quanto de sazonalidade nos dados. Ela é especialmente útil para séries que apresentam variações periódicas, permitindo modelar e prever valores futuros com maior precisão do que métodos simples, como a previsão naïve. O uso do SARIMA é importante porque ajuda na tomada de decisões baseada em padrões históricos complexos, permitindo antecipar demandas, custos ou recursos em diferentes contextos.\n"
      ],
      "metadata": {
        "id": "ryzos75mkikd"
      },
      "id": "ryzos75mkikd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cbd40b6",
      "metadata": {
        "id": "3cbd40b6"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Definição do espaço de busca da previsão SARIMA\n",
        "# ============================================================\n",
        "\n",
        "p = d = q = range(0, 2)                                         # parâmetros do componente ARIMA (p, d, q), assumindo valores de 0 a 2\n",
        "\n",
        "P = D = Q = range(0, 2)                                         # parâmetros do componente sazonal (P, D, Q, m), assumindo valores de 0 a 1 e 12 meses\n",
        "m = 12                                                          # sazonalidade anual (12 meses)\n",
        "\n",
        "# Todas as combinações possíveis\n",
        "pdq = list(itertools.product(p, d, q))                          # combinações de (p, d, q)\n",
        "seasonal_pdq = list(itertools.product(P, D, Q, [m]))            # combinações de (P, D, Q, m)\n",
        "\n",
        "# ============================================================\n",
        "# Busca pelo melhor modelo com base no critério AIC (Akaike Information Criterion)\n",
        "# ============================================================\n",
        "\n",
        "best_aic = np.inf                                               # inicializa com infinito (para otimizar encontrando o menor valor)\n",
        "best_order, best_seasonal = None, None\n",
        "best_model = None\n",
        "\n",
        "for order in pdq:                                               # Loop por todas as combinações possíveis de parâmetros\n",
        "    for seasonal_order in seasonal_pdq:\n",
        "        try:\n",
        "            # Ajusta o modelo SARIMA\n",
        "            model = sm.tsa.statespace.SARIMAX(\n",
        "                mmm_marinha['consumo'],                         # série temporal\n",
        "                order=order,                                    # parâmetros (p, d, q)\n",
        "                seasonal_order=seasonal_order,                  # parâmetros sazonais (P, D, Q, m)\n",
        "                enforce_stationarity=False                      # não força estacionariedade\n",
        "            )\n",
        "            results = model.fit(disp=False)\n",
        "\n",
        "            if results.aic < best_aic:                          # Atualiza se o modelo atual tiver melhor AIC\n",
        "                best_aic = results.aic\n",
        "                best_order, best_seasonal = order, seasonal_order\n",
        "                best_model = results\n",
        "\n",
        "        except Exception:\n",
        "            continue                                            # Ignora combinações que não convergem para o caso de Decomposition error\n",
        "\n",
        "# ============================================================\n",
        "# Resultado final da busca\n",
        "# ============================================================\n",
        "print(\n",
        "    f'Melhor modelo SARIMA encontrado: '\n",
        "    f'order={best_order}, seasonal_order={best_seasonal}'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Criação do modelo SARIMA com os melhores hiperparâmetros encontrados\n",
        "# ============================================================\n",
        "\n",
        "model = SARIMAX(\n",
        "    train.consumo,\n",
        "    order=best_order,                                           # order: parâmetros (p, d, q) para parte autorregressiva e de médias móveis\n",
        "    seasonal_order=best_seasonal,                               # seasonal_order: parâmetros sazonais (P, D, Q, m)\n",
        "    enforce_stationarity=False                                  # enforce_stationarity: desabilitado para maior flexibilidade\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# Ajuste do modelo aos dados de treino\n",
        "# ============================================================\n",
        "\n",
        "res = model.fit(disp=False, maxiter=500)                        # maxiter=500: limite de iterações para garantir a convergência\n",
        "\n",
        "# ============================================================\n",
        "# Exibição de métricas detalhadas do modelo ajustado\n",
        "# ============================================================\n",
        "\n",
        "print(res.summary().tables[1])                                  # a tabela traz os coeficientes e estatísticas de significância\n",
        "\n",
        "# ============================================================\n",
        "# Geração da previsão para o horizonte desejado (12 meses)\n",
        "# ============================================================\n",
        "\n",
        "pred = res.get_forecast(steps=12)\n",
        "y_pred_sarima = pred.predicted_mean                                    # valores previstos\n",
        "\n",
        "bias = abs(test.consumo - y_pred_sarima).mean()                        # cálculo do viés (tendência de super ou subestimar os valores reais)\n",
        "\n",
        "y_pred_sarima_no_bias = y_pred_sarima - bias                           # ajuste da previsão removendo o viés médio, para capturar melhor a variância das previsões no gráfico\n",
        "\n",
        "mae_sarima = mean_absolute_error(mmm_marinha.consumo.iloc[-12:], y_pred_sarima_no_bias)                # erro médio absoluto\n",
        "rmse_sarima = root_mean_squared_error(mmm_marinha.consumo.iloc[-12:], y_pred_sarima_no_bias)           # raiz do erro quadrático médio\n",
        "r2_sarima = r2_score(mmm_marinha.consumo.iloc[-12:], y_pred_sarima_no_bias)                            # coeficiente de determinação\n",
        "\n",
        "print(f\"Previsão SARIMA -> MAE: {mae_sarima:.2f} | RMSE: {rmse_sarima:.2f} | R2: {r2_sarima}\")"
      ],
      "metadata": {
        "id": "Ye0h5frgOGgT"
      },
      "id": "Ye0h5frgOGgT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"justify\">\n",
        "\n",
        "A variável que pode ser considerada significativa para o SARIMA foi apenas a média móvel da sazonalidade do lag de 12 meses.<br>\n",
        "Existe uma quantidade muito grande de ruídos."
      ],
      "metadata": {
        "id": "oYbW7y6nFJPD"
      },
      "id": "oYbW7y6nFJPD"
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plota_resultados(\n",
        "    title=\"Previsão temporal — SARIMA\",\n",
        "    preds={\n",
        "        \"SARIMA\": (y_pred_sarima_no_bias)\n",
        "    }\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "K71MGq_bfNgz"
      },
      "id": "K71MGq_bfNgz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "qqIYoFrQTRrO",
      "metadata": {
        "id": "qqIYoFrQTRrO"
      },
      "source": [
        "#### Exponential Smoothing (critério de comparação)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"justify\">\n",
        "\n",
        "A previsão Holt-Winters é um método de séries temporais que estende a suavização exponencial para capturar tendências e sazonalidades nos dados, usando três componentes: nível, tendência e sazonalidade. Ela é importante porque permite gerar previsões mais precisas em séries que apresentam padrões sazonais e mudanças graduais ao longo do tempo, sendo amplamente aplicada em planejamento de estoques, finanças e gestão de recursos, onde antecipar variações futuras é essencial para a tomada de decisão.\n"
      ],
      "metadata": {
        "id": "0BuweI54lQ7B"
      },
      "id": "0BuweI54lQ7B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4aec5d4b",
      "metadata": {
        "id": "4aec5d4b"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Configurações do modelo Holt-Winters\n",
        "# ============================================================\n",
        "\n",
        "hw_config = {\n",
        "    \"trend\": \"add\",                                             # modelo considera tendência aditiva (crescimento linear)\n",
        "    \"seasonal\": \"add\",                                          # sazonalidade aditiva (variações se somam à tendência)\n",
        "    \"seasonal_periods\": 12                                      # ciclo sazonal de 12 meses (dados mensais -> sazonalidade anual)\n",
        "}\n",
        "\n",
        "# ============================================================\n",
        "# Instanciação e ajuste do modelo Holt-Winters\n",
        "# ============================================================\n",
        "\n",
        "hw_model = ExponentialSmoothing(\n",
        "    mmm_marinha.set_index('ano_mes').iloc[:-12].values,         # série de treino\n",
        "    **hw_config                                                 # passa as configurações definidas acima\n",
        ").fit(\n",
        "    optimized=True,                                             # ajusta automaticamente os melhores parâmetros de suavização\n",
        "    use_brute=True                                              # força busca exaustiva para maior chance de encontrar parâmetros ótimos\n",
        ")\n",
        "\n",
        "\n",
        "forecast_horizon = 12                                           # horizonte de previsão (número de passos à frente)\n",
        "y_pred_hw = hw_model.forecast(steps=forecast_horizon)             # geração de previsões\n",
        "\n",
        "bias = abs(test.consumo - y_pred_hw).mean()                        # cálculo do viés (tendência de super ou subestimar os valores reais)\n",
        "\n",
        "y_pred_hw_no_bias = y_pred_hw - bias\n",
        "\n",
        "mae_hw = mean_absolute_error(mmm_marinha.consumo.iloc[-12:], y_pred_hw_no_bias)                # erro médio absoluto\n",
        "rmse_hw = root_mean_squared_error(mmm_marinha.consumo.iloc[-12:], y_pred_hw_no_bias)           # raiz do erro quadrático médio\n",
        "r2_hw = r2_score(mmm_marinha.consumo.iloc[-12:], y_pred_hw_no_bias)                            # coeficiente de determinação\n",
        "\n",
        "print(f\"Previsão Holt-Winters -> MAE: {mae_hw:.2f} | RMSE: {rmse_hw:.2f} | R2: {r2_hw}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d782840",
      "metadata": {
        "id": "8d782840"
      },
      "outputs": [],
      "source": [
        "fig = plota_resultados(\n",
        "    title=\"Previsão temporal — ExponentialSmoothing\",\n",
        "    preds={\n",
        "        \"ExponentialSmoothing\": (y_pred_hw_no_bias)\n",
        "    }\n",
        ")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Tfo6DUp-TBlL",
      "metadata": {
        "id": "Tfo6DUp-TBlL"
      },
      "source": [
        "#### Prophet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"justify\">\n",
        "\n",
        "A previsão Prophet é um modelo de séries temporais de alto nível desenvolvido pelo Facebook em 2017, projetado para lidar com dados com forte tendência e sazonalidade, incluindo feriados e eventos especiais. Este modelo combina uma abordagem aditiva, decompondo a série em componentes de tendência, sazonalidade e feriados, com métodos robustos de ajuste a outliers e mudanças abruptas. O Prophet é importante porque facilita a modelagem de séries complexas de forma automática e interpretável, permitindo previsões precisas mesmo com dados irregulares ou incompletos, sendo amplamente usado em negócios para planejamento financeiro, demanda e marketing.\n"
      ],
      "metadata": {
        "id": "qKUrod4plron"
      },
      "id": "qKUrod4plron"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "678e5898",
      "metadata": {
        "id": "678e5898"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Preparação dos dados para Prophet\n",
        "# ============================================================\n",
        "prophet_df = (\n",
        "    mmm_marinha                        # seleciona colunas relevantes\n",
        "    .rename(columns={'ano_mes': 'ds', 'consumo': 'y'})          # renomeia para padrão Prophet: ds -> data, y -> valor\n",
        "    .assign(ds=lambda d: pd.to_datetime(d['ds'], format='%m_%Y'))  # converte strings para datetime\n",
        "    .sort_values('ds')                                          # garante ordem cronológica\n",
        "    .reset_index(drop=True)                                     # reseta índice após ordenação\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# Configuração do modelo Prophet\n",
        "# ============================================================\n",
        "model_prophet = Prophet(\n",
        "    yearly_seasonality=True,                                    # ativa sazonalidade anual\n",
        "    weekly_seasonality=False,                                   # desativa sazonalidade semanal\n",
        "    daily_seasonality=False,                                    # desativa sazonalidade diária\n",
        "    seasonality_mode=\"additive\",                                # modelo aditivo (soma tendência + sazonalidade)\n",
        "    interval_width=0.95                                         # intervalo de confiança de 95% para previsão\n",
        ")\n",
        "\n",
        "# Ajuste do modelo aos dados de treino\n",
        "model_prophet.fit(prophet_df)\n",
        "\n",
        "# ============================================================\n",
        "# Criação do dataframe para previsão futura\n",
        "# ============================================================\n",
        "forecast_horizon = 12                                           # meses à frente\n",
        "future = model_prophet.make_future_dataframe(\n",
        "    periods=forecast_horizon,                                   # número de passos à frente\n",
        "    freq='M'                                                    # frequência mensal\n",
        ")\n",
        "\n",
        "# Geração das previsões\n",
        "forecast = model_prophet.predict(future)\n",
        "\n",
        "# ============================================================\n",
        "# Seleção das previsões correspondentes ao conjunto de teste\n",
        "# ============================================================\n",
        "test_dates = pd.to_datetime(test['ano_mes'], format='%m_%Y')    # datas do teste\n",
        "forecast_test = forecast.set_index('ds').loc[test_dates]        # filtra apenas o horizonte de teste\n",
        "\n",
        "y_true = mmm_marinha.consumo.iloc[-12:]                # valores reais\n",
        "y_pred_prophet = forecast_test['yhat']  # valores previstos\n",
        "\n",
        "# ============================================================\n",
        "# Avaliação do modelo\n",
        "# ============================================================\n",
        "\n",
        "mae_prophet = mean_absolute_error(mmm_marinha.consumo.iloc[-12:], y_pred_prophet)                # erro médio absoluto\n",
        "rmse_prophet = root_mean_squared_error(mmm_marinha.consumo.iloc[-12:], y_pred_prophet)           # raiz do erro quadrático médio\n",
        "r2_prophet = r2_score(mmm_marinha.consumo.iloc[-12:], y_pred_prophet)                            # coeficiente de determinação\n",
        "\n",
        "clear_output(wait=True)\n",
        "\n",
        "print(f\"\\n\\nPrevisão Prophet -> MAE: {mae_prophet:.2f} | RMSE: {rmse_prophet:.2f} | R2: {r2_prophet}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f7c19f3",
      "metadata": {
        "id": "2f7c19f3"
      },
      "outputs": [],
      "source": [
        "fig = plota_resultados(\n",
        "    title=\"Previsão temporal — Prophet\",\n",
        "    preds={\n",
        "        \"Prophet\": (y_pred_prophet)\n",
        "    }\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2toQJB6hTFbt",
      "metadata": {
        "id": "2toQJB6hTFbt"
      },
      "source": [
        "#### XGBoost regressor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"justify\">\n",
        "\n",
        "O XGBoost Regressor é um modelo de aprendizado de máquina baseado em **gradient boosting** (e por isso o nome) que combina múltiplas árvores de decisão para reduzir erros e melhorar a precisão da previsão. Aplicado a séries temporais, ele utiliza variáveis defasadas e outras features derivadas do tempo para capturar padrões complexos não lineares que métodos tradicionais podem não identificar. É importante para análise de séries temporais porque permite modelar relações intricadas entre variáveis, lidar com grandes volumes de dados e gerar previsões mais precisas, sendo amplamente utilizado em finanças, demanda de produtos e manutenção preditiva.\n"
      ],
      "metadata": {
        "id": "UMxkJeCdmC5J"
      },
      "id": "UMxkJeCdmC5J"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b9e0724",
      "metadata": {
        "id": "0b9e0724"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Preparação de features para XGBoost\n",
        "# ============================================================\n",
        "xg_df = mmm_marinha[['consumo']].copy()                         # série alvo\n",
        "\n",
        "for lag in [1, 3, 6]:                                           # Criação de lags\n",
        "    xg_df[f'lag{lag}'] = xg_df['consumo'].shift(lag)            # adiciona colunas com valores passados da série (lags)\n",
        "\n",
        "\n",
        "for window in [3, 6]:                                           # Criação de médias móveis (rolling)\n",
        "    xg_df[f'rolling{window}'] = xg_df['consumo'].rolling(window).mean() # adiciona colunas com média móvel da série, para capturar tendências locais\n",
        "\n",
        "\n",
        "xg_df = xg_df.dropna().reset_index(drop=True)                   # Remove linhas com NaN gerados pelos lags e médias móveis e reseta índice\n",
        "\n",
        "# ============================================================\n",
        "# Separação treino / teste\n",
        "# ============================================================\n",
        "horizon = 12                                                    # últimos 12 períodos serão usados como teste\n",
        "train_xg, test_xg = xg_df.iloc[:-horizon], xg_df.iloc[-horizon:]\n",
        "\n",
        "X_train, y_train = train_xg.drop(columns=[\"consumo\"]), train_xg[\"consumo\"]  # features\n",
        "X_test, y_test   = test_xg.drop(columns=[\"consumo\"]), test_xg[\"consumo\"]    # target\n",
        "\n",
        "# ============================================================\n",
        "# Configuração do modelo XGBoost\n",
        "# ============================================================\n",
        "xgb_params = dict(\n",
        "    n_estimators=300,                                           # número de árvores\n",
        "    learning_rate=0.05,                                         # taxa de aprendizado\n",
        "    max_depth=5,                                                # profundidade máxima de cada árvore\n",
        "    subsample=0.8,                                              # amostragem de linhas para cada árvore\n",
        "    colsample_bytree=0.8,                                       # amostragem de colunas para cada árvore\n",
        "    random_state=42,                                            # reprodutibilidade\n",
        "    n_jobs=-1,                                                  # usa todos os núcleos disponíveis\n",
        "    objective=\"reg:squarederror\",                               # objetivo de regressão\n",
        "    verbosity=0                                                 # sem logs de treino\n",
        ")\n",
        "\n",
        "xgb_model = XGBRegressor(**xgb_params)\n",
        "\n",
        "# ============================================================\n",
        "# Treinamento do modelo\n",
        "# ============================================================\n",
        "\n",
        "xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False) # fornecendo eval_set apenas para monitoramento (não será usado na métrica final, evitando dat leakage)\n",
        "\n",
        "# ============================================================\n",
        "# Geração de previsões e avaliação\n",
        "# ============================================================\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)                # erro médio absoluto\n",
        "rmse_xgb = root_mean_squared_error(y_test, y_pred_xgb)           # raiz do erro quadrático médio\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)                            # coeficiente de determinação\n",
        "\n",
        "print(f\"XGBoost — MAE: {mae_xgb:.2f} | RMSE: {rmse_xgb:.2f} | R2: {r2_xgb}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "610a55a9",
      "metadata": {
        "id": "610a55a9"
      },
      "outputs": [],
      "source": [
        "fig = plota_resultados(\n",
        "    title=\"Previsão temporal — XGBoost\",\n",
        "    preds={\n",
        "        \"XGBoost\": (y_pred_xgb)\n",
        "    }\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mV8L6DMLTW-T",
      "metadata": {
        "id": "mV8L6DMLTW-T"
      },
      "source": [
        "#### LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"justify\">\n",
        "\n",
        "A previsão LSTM (Long Short-Term Memory) é um tipo de rede neural recorrente desenvolvido para superar o problema do desvanecimento do gradiente em sequências longas, permitindo que informações relevantes sejam lembradas por períodos prolongados. Aplicada a séries temporais, a LSTM é capaz de capturar dependências temporais complexas e padrões não lineares, aprendendo automaticamente relações de curto e longo prazo nos dados. Ela é importante para análise de séries temporais porque oferece alta precisão em previsões de séries complexas, como financeiras, de demanda ou de sensores, onde métodos tradicionais têm dificuldade de modelar dinâmicas temporais sutis.\n"
      ],
      "metadata": {
        "id": "y3KfiF7DmeOS"
      },
      "id": "y3KfiF7DmeOS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a3bfe4c",
      "metadata": {
        "id": "1a3bfe4c"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Normalização dos dados da série temporal\n",
        "# ============================================================\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))                     # escalamos os dados para o intervalo [0,1] para ajudar a estabilizar o gradiente durante o treinamento da rede neural\n",
        "despesas_scaled = scaler.fit_transform(\n",
        "    mmm_marinha.consumo.values.reshape(-1, 1)                   # reshape para matriz 2D (requisito para o MinMaxScaler)\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# Função para criar janelas temporais (sequências)\n",
        "# ============================================================\n",
        "\n",
        "def create_sequences(data, window=12):\n",
        "    \"\"\"\n",
        "    Constrói sequências de tamanho 'window' para predição de séries temporais.\n",
        "\n",
        "    Args:\n",
        "        data (array): série temporal escalada\n",
        "        window (int): número de passos no histórico usados para prever o próximo valor\n",
        "\n",
        "    Returns:\n",
        "        X (array): entradas, cada linha é uma janela de 'window' passos\n",
        "        y (array): saídas, cada valor é o próximo ponto a ser previsto\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - window):\n",
        "        X.append(data[i : i + window])                          # pega dados em uma janela de tamanho 'window'\n",
        "        y.append(data[i + window])                              # o alvo é o valor logo após a janela\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X, y = create_sequences(despesas_scaled)                        # criação das sequências a partir da série normalizada\n",
        "\n",
        "# ============================================================\n",
        "# Separação treino / teste\n",
        "# ============================================================\n",
        "\n",
        "split = len(X) - 12                                             # últimos 12 pontos reservados para teste\n",
        "X_train, X_test = X[:split], X[split:]\n",
        "y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "print('Shape treino (X, y):', X_train.shape, y_train.shape)     # checando os formatos dos conjuntos criados para alimentar a rede neural\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd4973a6",
      "metadata": {
        "id": "dd4973a6"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Definição do modelo LSTM\n",
        "# ============================================================\n",
        "\n",
        "lstm_model = Sequential([\n",
        "\n",
        "    LSTM(128,                                                   # primeira camada LSTM com 128 neurônios\n",
        "         activation='relu',                                     # activation 'ReLu para garantir a não-linearidade\n",
        "         return_sequences=True,                                 # return_sequences para empilhar com outra LSTM\n",
        "         input_shape=(12, 1)),                                  # janela de 12 steps e uma feature, de acordo com o shape dos dados de entrada\n",
        "\n",
        "    Dropout(1e-2),\n",
        "\n",
        "    BatchNormalization(),                                       # normaliza ativações, acelerando o treinamento e evitando explosão/desaparecimento do gradiente\n",
        "\n",
        "    LSTM(64,                                                    # segunda camada LSTM com 64 neurônios\n",
        "         activation='relu',\n",
        "         return_sequences=False),                               # return_sequences falso, pois é a última camada de LSTM, retornando apenas o último estado\n",
        "\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Dense(64, activation='relu'),                               # camadas densas totalmente conectadas para refinar padrões temporais\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(16, activation='relu'),\n",
        "\n",
        "    Dense(1)                                                    # saída com 1 neurônio: previsão de um único valor contínuo\n",
        "])\n",
        "\n",
        "# ============================================================\n",
        "# Compilação do modelo\n",
        "# ============================================================\n",
        "\n",
        "lstm_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Nadam(learning_rate=1e-3),    # otimizador Nadam com taxa de aprendizado inicial\n",
        "    loss=tf.keras.losses.MeanSquaredLogarithmicError(),         # Função de perda: MAE (erro absoluto médio), comum em séries temporais\n",
        "    metrics=['mse']                                             # Métrica: MSE (erro quadrático médio), para avaliação adicional\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# Callbacks para controle de treinamento\n",
        "# ============================================================\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=200, restore_best_weights=True, verbose=1),  # para o treinamento cedo se não houver melhora na validação\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=50, min_lr=1e-5, verbose=1) # reduz a taxa de aprendizado se a validação estagnar\n",
        "]\n",
        "\n",
        "# ============================================================\n",
        "# Treinamento do modelo\n",
        "# ============================================================\n",
        "\n",
        "history = lstm_model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=2000,                                                # treinamento em 1000 épocas (pode ser interrompido antes via EarlyStopping)\n",
        "    batch_size=16,                                              # batch de tamanho 16 (quantidade de sequências que o modelo vê antes de cada atualização de gradiente)\n",
        "    validation_data=(X_test, y_test),                           # dados do conjunto de testes para validação\n",
        "    callbacks=callbacks,                                        # callbacks ativados para evitar desperdício de tempo no treinamento e devolver o melhor resultado conhecido\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "clear_output(wait=True)\n",
        "\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df.sort_values(by='val_loss', ascending=False).tail()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ab14322",
      "metadata": {
        "id": "8ab14322"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Geração das previsões com o modelo LSTM\n",
        "# ============================================================\n",
        "\n",
        "y_pred_norm = lstm_model.predict(X_test)                        # obtém previsões no mesmo espaço normalizado usado no treino\n",
        "\n",
        "# ============================================================\n",
        "# Inversão do escalonamento (voltar valores para escala original)\n",
        "# ============================================================\n",
        "\n",
        "y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))    # \"desnormalizando\" y_test para comparação com os resultados da predição do modelo\n",
        "y_pred_lstm = scaler.inverse_transform(y_pred_norm)             # \"desnormalizando\" y_pred também para que eles retomem a amplitude dos dados originais\n",
        "\n",
        "# ============================================================\n",
        "# Avaliação do desempenho\n",
        "# ============================================================\n",
        "\n",
        "mae_lstm = mean_absolute_error(y_test_inv, y_pred_lstm)         # erro médio absoluto\n",
        "rmse_lstm = root_mean_squared_error(y_test_inv, y_pred_lstm)    # raiz do erro quadrático médio\n",
        "r2_lstm = r2_score(y_test_inv, y_pred_lstm)                     # coeficiente de determinação\n",
        "\n",
        "print(f\"Previsão LSTM -> MAE: {mae_lstm:.2f} | RMSE: {rmse_lstm:.2f} | R2: {r2_lstm}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfc0090f",
      "metadata": {
        "id": "bfc0090f"
      },
      "outputs": [],
      "source": [
        "fig = plota_resultados(\n",
        "    title=\"Previsão temporal — LSTM\",\n",
        "    preds={\n",
        "        \"LSTM\": (y_pred_lstm.flatten())\n",
        "    }\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5M6EqDdRTalL",
      "metadata": {
        "id": "5M6EqDdRTalL"
      },
      "source": [
        "#### Multistep LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"justify\">\n",
        "\n",
        "A principal diferença entre o modelo LSTM tradicional e o modelo **Multi-Step LSTM** está no horizonte de previsão. O LSTM padrão normalmente prevê **um passo à frente** na série temporal, ou seja, estima o próximo valor com base nos dados anteriores. Já o Multi-Step LSTM é projetado para prever **vários passos futuros de uma vez**, produzindo uma sequência de valores futuros em vez de apenas um. Isso é especialmente útil quando se deseja planejamento ou tomada de decisão com antecedência, mas também aumenta a complexidade do modelo, exigindo técnicas adicionais para lidar com erros que se propagam ao longo dos passos de previsão.\n"
      ],
      "metadata": {
        "id": "ePDKPQ7wm5_A"
      },
      "id": "ePDKPQ7wm5_A"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdf8a44c",
      "metadata": {
        "id": "bdf8a44c"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Função para criar janelas de treino com previsão multi-step\n",
        "# ============================================================\n",
        "\n",
        "def create_sequences_multistep(data, window=12, horizon=12):\n",
        "    \"\"\"\n",
        "    Gera pares (X, y) para treinamento de modelos de séries temporais multistep.\n",
        "\n",
        "    Args:\n",
        "        data (array): série temporal escalada ou normalizada\n",
        "        window (int): número de períodos usados como entrada (janelas passadas)\n",
        "        horizon (int): número de períodos futuros a serem previstos\n",
        "\n",
        "    Returns:\n",
        "        X (np.array): sequências de entrada (amostras × janela × features)\n",
        "        y (np.array): valores futuros (amostras × horizonte)\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "\n",
        "    for i in range(len(data) - window - horizon + 1):           # percorre a série até onde é possível formar uma janela completa + horizonte\n",
        "        X.append(data[i:i+window])                              # janela de entrada (ex.: últimos 12 meses)\n",
        "        y.append(data[i+window:i+window+horizon].flatten())     # horizonte de saída (ex.: próximos 12 meses)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Criação dos conjuntos de dados\n",
        "# ============================================================\n",
        "\n",
        "window = 12                                                     # janela de 12 meses (1 ano de histórico para prever)\n",
        "horizon = 12                                                    # previsão de 12 meses à frente\n",
        "X, y = create_sequences_multistep(despesas_scaled, window, horizon)\n",
        "\n",
        "# ============================================================\n",
        "# Divisão treino/teste\n",
        "# ============================================================\n",
        "\n",
        "split = len(X) - horizon                                        # separação dos dados de treino e teste\n",
        "X_train, X_test = X[:split], X[split:]\n",
        "y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "# ============================================================\n",
        "# Verificação dos shapes\n",
        "# ============================================================\n",
        "\n",
        "print('X_train shape:', X_train.shape)                          # (amostras, janela, features)\n",
        "print('y_train shape:', y_train.shape)                          # (amostras, horizonte)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d401850",
      "metadata": {
        "id": "0d401850"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Definição da arquitetura da rede LSTM para previsão multi-step\n",
        "# ============================================================\n",
        "\n",
        "model = Sequential([\n",
        "\n",
        "    LSTM(128,                                                   # primeira camada LSTM (captura padrões temporais mais longos)\n",
        "         activation='relu',\n",
        "         return_sequences=True,\n",
        "         input_shape=(window, 1)\n",
        "         ),\n",
        "    Dropout(1e-2),\n",
        "\n",
        "    BatchNormalization(),                                       # normaliza a saída da camada LSTM para estabilizar o treinamento\n",
        "\n",
        "\n",
        "    LSTM(64,                                                    # segunda camada LSTM (captura padrões mais refinados, sem retornar sequência completa)\n",
        "         activation='relu',\n",
        "         return_sequences=False\n",
        "         ),\n",
        "    Dropout(1e-2),\n",
        "\n",
        "    BatchNormalization(),\n",
        "\n",
        "\n",
        "    Dense(64, activation='relu'),                               # Camadas densas totalmente conectadas para refinar o aprendizado\n",
        "\n",
        "    Dropout(1e-2),\n",
        "\n",
        "    Dense(32, activation='relu'),\n",
        "\n",
        "    Dropout(1e-2),\n",
        "\n",
        "    Dense(horizon)                                              # Camada de saída com dimensão igual ao horizonte: previsão multi-step 12 passos à frente\n",
        "])\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Compilação do modelo\n",
        "# ============================================================\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Nadam(learning_rate=1e-3),\n",
        "    loss=tf.keras.losses.MeanSquaredLogarithmicError(),         # erro quadrático médio (mais sensível a grandes desvios)\n",
        "    metrics=['mse']                                             # erro absoluto médio (interpretação direta em unidades originais)\n",
        ")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Callbacks para controle do treinamento\n",
        "# ============================================================\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',                                     # monitora a perda de validação\n",
        "        patience=200,                                           # interrompe se não houver melhora após 200 épocas\n",
        "        restore_best_weights=True,                              # restaura os melhores pesos obtidos\n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',                                     # reduz a taxa de aprendizado se o modelo 'empacar'\n",
        "        factor=0.5,                                             # reduz pela metade\n",
        "        patience=50,                                           # espera 200 épocas sem melhora\n",
        "        min_lr=1e-5,                                            # limite mínimo de taxa de aprendizado\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Treinamento do modelo\n",
        "# ============================================================\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=2000,                                                # número máximo de épocas (propositalmente 'alto', porque early stopping pode parar antes)\n",
        "    batch_size=16,                                              # tamanho do lote (menor => mais ruído, maior => gradiente mais estável)\n",
        "    validation_data=(X_test, y_test),                           # validação em dados de teste\n",
        "    callbacks=callbacks,                                        # usa callbacks definidos acima\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "clear_output(wait=True)\n",
        "\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df.sort_values(by='val_loss', ascending=False).tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07ef65ad",
      "metadata": {
        "id": "07ef65ad"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Geração de previsões com o modelo treinado\n",
        "# ============================================================\n",
        "\n",
        "y_pred = model.predict(X_test)                                  # previsões para o conjunto de teste\n",
        "\n",
        "# ============================================================\n",
        "# Inversão da normalização\n",
        "# ============================================================\n",
        "\n",
        "y_test_inv = scaler.inverse_transform(y_test)                   # valores reais (teste)\n",
        "y_pred_multi_lstm = scaler.inverse_transform(y_pred)            # valores previstos\n",
        "\n",
        "# ============================================================\n",
        "# Avaliação da performance\n",
        "# ============================================================\n",
        "\n",
        "mae_multi_lstm = mean_absolute_error(y_test_inv[-1], y_pred_multi_lstm[-1])         # erro médio absoluto\n",
        "rmse_multi_lstm = root_mean_squared_error(y_test_inv[-1], y_pred_multi_lstm[-1])    # raiz do erro quadrático médio\n",
        "r2_multi_lstm = r2_score(y_test_inv[-1], y_pred_multi_lstm[-1])                     # coeficiente de determinação\n",
        "\n",
        "print(f\"Previsão multi LSTM -> MAE: {mae_multi_lstm:.2f} | RMSE: {rmse_multi_lstm:.2f} | R2: {r2_multi_lstm}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a3ae83a",
      "metadata": {
        "id": "5a3ae83a"
      },
      "outputs": [],
      "source": [
        "fig = plota_resultados(\n",
        "    title=\"Previsão temporal — Multi-step LSTM\",\n",
        "    preds={\n",
        "        \"Multi-step LSTM\": (y_pred_multi_lstm[-1])\n",
        "    }\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D1MgfHeFXe6s",
      "metadata": {
        "id": "D1MgfHeFXe6s"
      },
      "source": [
        "#### N-Beats"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"justify\">\n",
        "\n",
        "A previsão N-BEATS (Neural Basis Expansion Analysis for Time Series), desenvolvida em [2019](https://arxiv.org/abs/1905.10437), é um modelo de redes neurais totalmente conectado projetado especificamente para séries temporais, que decompõe os dados em componentes de tendência e sazonalidade usando blocos de base adaptativos. Diferentemente de LSTM ou modelos autoregressivos, o N-BEATS não depende de estruturas recorrentes ou convolucionais, o que simplifica o treinamento e melhora a interpretabilidade. Ele é importante porque oferece alta precisão em previsões de curto e longo prazo, é capaz de lidar com séries temporais complexas e irregulares, e tem se mostrado competitivo em benchmarks de previsão.\n"
      ],
      "metadata": {
        "id": "vejOfmh4nHsw"
      },
      "id": "vejOfmh4nHsw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6cda0ff",
      "metadata": {
        "id": "e6cda0ff"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Preparação dos dados para o N-BEATS\n",
        "# ============================================================\n",
        "\n",
        "train['item_id'] = 'mnc'                                        # identificador único da série temporal\n",
        "nbeats_data = train.rename(                                     # o NeuralForecast exige colunas com nomes específicos\n",
        "    columns={\n",
        "        'item_id': 'unique_id',                                 # unique_id: identifica a série (mesmo que haja apenas uma)\n",
        "        'ano_mes': 'ds',                                        # ds: datas (formato datetime)\n",
        "        'consumo': 'y'                                          # y: valores observados (variável alvo)\n",
        "        }\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# Configuração do modelo N-BEATS\n",
        "# ============================================================\n",
        "\n",
        "model = NBEATS(\n",
        "    h=12,                                                       # h: horizonte de previsão (quantos passos à frente prever)\n",
        "    input_size=12,                                              # input_size: quantidade de observações usadas como entrada\n",
        "    stack_types=['seasonality', 'trend', 'identity'],           # stack_types: tipos de blocos (tendência, sazonalidade, identidade)\n",
        "    n_blocks=[3, 3, 3],                                         # n_blocks: número de blocos em cada pilha\n",
        "    activation='ReLU',\n",
        "\n",
        "    learning_rate=1e-3,                                         # learning_rate: taxa de aprendizado\n",
        "    num_lr_decays=3,                                            # num_lr_decays: número de reduções de LR automáticas\n",
        "    batch_size=16,                                              # batch_size: tamanho do lote (impacta velocidade/estabilidade do treino)\n",
        "    scaler_type='robust',                                       # scaler_type: tipo de normalização (\"robust\" lida melhor com outliers)\n",
        "\n",
        "    max_steps=1000,                                             # max_steps: limite de iterações no treinamento\n",
        "    val_check_steps=10,                                         # val_check_steps: frequência de checagem no conjunto de validação\n",
        "    early_stop_patience_steps=20                                # early_stop_patience_steps: paciência para early stopping\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# Treinamento do modelo\n",
        "# ============================================================\n",
        "\n",
        "nbeats_forecast = NeuralForecast(models=[model], freq='M')      # freq='M' indica que a série é mensal\n",
        "nbeats_forecast.fit(df=nbeats_data, val_size=12, verbose=False) # val_size: reserva de 12 meses para validação\n",
        "\n",
        "# ============================================================\n",
        "# Geração das previsões\n",
        "# ============================================================\n",
        "\n",
        "y_pred_nbeats = nbeats_forecast.predict()                       # Retorna as previsões multi-step para o horizonte definido\n",
        "\n",
        "# ============================================================\n",
        "# Avaliação das previsões\n",
        "# ============================================================\n",
        "\n",
        "mae_nbeats = mean_absolute_error(nbeats_data.y[-12:], y_pred_nbeats.NBEATS.values.flatten())         # erro médio absoluto\n",
        "rmse_nbeats = root_mean_squared_error(nbeats_data.y[-12:], y_pred_nbeats.NBEATS.values.flatten())    # raiz do erro quadrático médio\n",
        "r2_nbeats = r2_score(nbeats_data.y[-12:], y_pred_nbeats.NBEATS.values.flatten())                     # coeficiente de determinação\n",
        "\n",
        "clear_output(wait=True)\n",
        "\n",
        "print(f\"Previsão NBeats -> MAE: {mae_nbeats:.2f} | RMSE: {rmse_nbeats:.2f} | R2: {r2_nbeats}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plota_resultados(\n",
        "    title=\"Previsão temporal — N-Beats\",\n",
        "    preds={\n",
        "        \"N-Beats\": (y_pred_nbeats.NBEATS.values.flatten())\n",
        "    }\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "vTEP6ZTytjB8"
      },
      "id": "vTEP6ZTytjB8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Autogluon"
      ],
      "metadata": {
        "id": "msJ331VFVejD"
      },
      "id": "msJ331VFVejD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"justify\">\n",
        "\n",
        "A previsão AutoGluon para séries temporais é uma abordagem de aprendizado automático desenvolvido pela Amazon em 2020, que automatiza a construção, seleção e combinação de modelos preditivos, incluindo regressão, árvores de decisão e redes neurais. Para séries temporais, o AutoGluon cria múltiplos modelos candidatos e realiza **ensembles automáticos** para otimizar a precisão das previsões, lidando com tendências, sazonalidade e variáveis externas. Ele é importante porque reduz a complexidade e o tempo de desenvolvimento de modelos preditivos, permitindo gerar previsões confiáveis de forma rápida e escalável.\n"
      ],
      "metadata": {
        "id": "3MjY79hBoGu6"
      },
      "id": "3MjY79hBoGu6"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Preparação dos dados para o AutoGluon\n",
        "# ============================================================\n",
        "\n",
        "autogluon_data = nbeats_data.rename(                            # reaproveitando os dados utilizados para treinamento do NBeats (pois são de formato parecido)\n",
        "    columns={                                                   # o AutoGluon espera as seguintes colunas:\n",
        "        'unique_id': 'item_id',                                 # item_id: identifica a série temporal\n",
        "        'ds': 'timestamp',                                      # timestamp: data/hora dos registros (datetime)\n",
        "        'y': 'target'                                           # target: valores observados (variável alvo)\n",
        "        }\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# Definição do horizonte de previsão\n",
        "# ============================================================\n",
        "\n",
        "prediction_length = 12                                          # prediction_length: número de passos à frente para prever\n",
        "\n",
        "train_df = autogluon_data.iloc[:-prediction_length].copy()      # separação simples entre treino e teste\n",
        "test_df  = autogluon_data.iloc[-prediction_length:].copy()\n",
        "\n",
        "# ============================================================\n",
        "# Criação de subconjuntos para treino/validação\n",
        "# ============================================================\n",
        "\n",
        "val_size = prediction_length                                    # val_size: tamanho do conjunto de validação\n",
        "train_for_fit = autogluon_data.iloc[:-val_size].copy()\n",
        "tune_for_fit  = autogluon_data.iloc[-val_size - prediction_length : -prediction_length].copy()  # tune_for_fit: dados imediatamente antes do teste (usados em ajuste fino)\n",
        "\n",
        "# ============================================================\n",
        "# Instanciação do AutoGluon TimeSeriesPredictor\n",
        "# ============================================================\n",
        "\n",
        "predictor = TimeSeriesPredictor(\n",
        "    target='target',                                            # target: variável a ser prevista\n",
        "    prediction_length=prediction_length,                        # prediction_length: horizonte da previsão\n",
        "    eval_metric='MAE'                                           # eval_metric: métrica de avaliação primária\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# Treinamento do AutoGluon\n",
        "# ============================================================\n",
        "\n",
        "predictor.fit(\n",
        "    train_data=train_for_fit,\n",
        "    presets='best_quality',                                     # presets='best_quality': busca modelos mais robustos (mesmo que mais lentos)\n",
        "    time_limit=600,                                             # time_limit: limite de 10 minutos para o professor não perder a paciência durante a correção\n",
        "    verbosity=0                                                 # verbosity: nível de detalhamento do log\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# Geração das previsões\n",
        "# ============================================================\n",
        "\n",
        "autogluon_predictions = predictor.predict(autogluon_data)       # aqui usamos o conjunto completo e o AutoGluon automaticamente deduz o ponto de corte a partor do 'prediction_length'\n",
        "y_pred_autogluon = autogluon_predictions['mean'].values\n",
        "\n",
        "# ============================================================\n",
        "# Avaliação das previsões\n",
        "# ============================================================\n",
        "\n",
        "mae_autogluon = mean_absolute_error(autogluon_data.target[-12:], y_pred_autogluon)         # erro médio absoluto\n",
        "rmse_autogluon = root_mean_squared_error(autogluon_data.target[-12:], y_pred_autogluon)    # raiz do erro quadrático médio\n",
        "r2_autogluon = r2_score(autogluon_data.target[-12:], y_pred_autogluon)                     # coeficiente de determinação\n",
        "\n",
        "clear_output(wait=True)\n",
        "\n",
        "print(f\"Previsão Autogluon -> MAE: {mae_autogluon:.2f} | RMSE: {rmse_autogluon:.2f} | R2: {r2_autogluon}\")"
      ],
      "metadata": {
        "id": "iUGj0T7uVjOo"
      },
      "id": "iUGj0T7uVjOo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plota_resultados(\n",
        "    title=\"Previsão temporal — Autogluon\",\n",
        "    preds={\n",
        "        \"Autogluon\": (y_pred_autogluon)\n",
        "    }\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "VgX3VA0TWujd"
      },
      "id": "VgX3VA0TWujd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "MmTXRrVdTith",
      "metadata": {
        "id": "MmTXRrVdTith"
      },
      "source": [
        "### Seleção do melhor modelo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Métricas dos modelos treinados\n",
        "\n",
        "#@markdown <div align=\"justify\"> As métricas MAE, RMSE (ou MSE) e R² porque elas são complementares e fornecem uma avaliação abrangente do desempenho de modelos de regressão aplicados a séries temporais.\n",
        "#@markdown <br> - O MAE (Mean Absolute Error) mede o erro médio absoluto na mesma unidade da variável de interesse, sendo intuitivo e de fácil interpretação prática, pois mostra em média “quanto se erra” nas previsões.\n",
        "#@markdown <br> - O MSE (Mean Squared Error) penaliza mais fortemente grandes erros devido à elevação ao quadrado, sendo adequado para cenários em que erros extremos são particularmente indesejados.\n",
        "#@markdown <br> - Já o R² (Coeficiente de Determinação) avalia a proporção da variabilidade da série explicada pelo modelo em comparação a uma previsão ingênua baseada na média, funcionando como um indicador de ajuste global.\n",
        "#@markdown <br><br> Em conjunto, essas métricas permitem analisar não apenas a magnitude dos erros, mas também a qualidade geral do ajuste do modelo, garantindo uma avaliação equilibrada e consistente.\n",
        "\n",
        "\n",
        "modelos_eval = pd.DataFrame(\n",
        "    columns = ['MAE', 'RMSE', 'R2'],\n",
        "    index = ['naive', 'sarima', 'hw', 'prophet', 'xgb', 'lstm', 'multi_lstm', 'nbeats', 'autogluon']\n",
        ")\n",
        "\n",
        "for idx in modelos_eval.index:\n",
        "    for col in modelos_eval.columns:\n",
        "      modelos_eval.loc[idx, col] = eval(f'{col.lower()}_{idx}')\n",
        "\n",
        "modelos_eval.sort_values(by='R2', ascending=False)"
      ],
      "metadata": {
        "id": "HqOk9Wwhhvd8",
        "cellView": "form"
      },
      "id": "HqOk9Wwhhvd8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"justify\">\n",
        "\n",
        "#### Considerações sobre os modelos treinados\n",
        "\n",
        "**Naïve**: modelo sem nenhuma complexidade envolvida. Foi contemplado aqui por apresentar minimamente alguma precisão, ao repetir para a próxima observação o valor da última. Qualquer outro modelo mais complexo que não atinja este resultado mínimo não tem a sua complexidade justificada.\n",
        "\n",
        "**SARIMA**: Apesar de não ser propriamente um modelo de aprendizado de máquina, mas sim um modelo estatístico de séries temporais baseado na teoria de autoregressão, é muito efetivo e amplamente utilizado. Depende de uma parametrização explícita que foi otimizada em um loop que comparava diferentes ajustes ao se medir o AIC. Apresentou um bom resultado com erros menores do que o modelo Naïve.\n",
        "\n",
        "**Holt-Winters**: Assim como o SARIMA, se trata de um modelo estatístico de séries temporais, porém baseando-se na suavização exponencial, ajustando os próprios parâmetros de forma determinística. Também apresentou resultados razoáveis com erros abaixo do modelo Naïve.\n",
        "\n",
        "**Prophet**: Modelo desenvolvido pelo Facebook, baseado em decomposição aditiva, que combina tendência, sazonalidade e efeitos de feriados. Mostrou desempenho intermediário, superior ao Naïve e Holt-Winters, mas inferior ao XGBoost e similar ao SARIMA, com capacidade de capturar padrões sazonais de forma relativamente simples e interpretável.\n",
        "\n",
        "**XGBoost Regressor**: Foi o modelo que obteve o melhor desempenho entre todos, com os menores valores de MAE e RMSE e o maior R². Isso indica que conseguiu capturar relações não lineares complexas da série com mais eficiência que os modelos estatísticos e até mesmo que arquiteturas de deep learning testadas, além de ter tomado um tempo muito menor para treinamento, sendo a solução mais adequada para o caso em tela.\n",
        "\n",
        "**LSTM**: Rede neural recorrente capaz de capturar dependências de curto e longo prazo em séries temporais. Apresentou ótimo desempenho, com métricas que indicam bom ajuste, abaixo apenas do XGBoost. Possivelmente este resultado pode ser ainda melhor mediante um criterioso tuning de hiperparâmetros, mais dados ou engenharia de features temporal mais sofisticada. No entanto, avaliando o custo-beneficio da complexidade do modelo e seus resultados, considera-se o XGBoost regressor uma escolha melhor.\n",
        "\n",
        "**Multi-step LSTM**: Modelo configurado para prever múltiplos passos à frente em uma única execução. Neste caso, apresentou resultados bastante ruins, com erros significativamente maiores que os demais modelos. Isso sugere que a arquitetura ou a parametrização não conseguiu lidar bem com a tarefa, tornando o modelo ineficiente neste cenário. Mesmo sendo testados diferentes hiperparâmetros, com diferentes otimizadores, o resultado não foi satisfatório.\n",
        "\n",
        "**N-Beats**: Rede neural desenvolvida especificamente para previsão de séries temporais. Apesar de seu ótimo desempenho reconhecido, neste conjunto apresentou erros elevados, ficando abaixo até mesmo do modelo Naïve. Isso pode ter ocorrido por limitação de dados, ajustes inadequados ou características específicas da série que dificultaram a generalização.\n",
        "\n",
        "**AutoGluon**: Framework de AutoML que combina diversos modelos em ensemble. Esperava-se que tivesse desempenho competitivo, mas neste caso apresentou resultados insatisfatórios, inclusive inferiores ao Naïve, com R² negativo. Isso reforça que, para este problema, abordagens automáticas generalistas não superaram a modelagem customizada e específica da série temporal."
      ],
      "metadata": {
        "id": "QLubAlUAt2Ay"
      },
      "id": "QLubAlUAt2Ay"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Comparação dos modelos\n",
        "fig = plota_resultados(\n",
        "    df=mmm_marinha[-12:],\n",
        "    title=\"Previsão temporal — Comparação de todos os modelos treinados\",\n",
        "    preds={\n",
        "        \"Naïve\": (y_pred_naive),\n",
        "        \"SARIMA\": (y_pred_sarima_no_bias),\n",
        "        \"Holt-Winters\": (y_pred_hw),\n",
        "        \"Prophet\": (y_pred_prophet),\n",
        "        \"XGBoost\": (y_pred_xgb),\n",
        "        \"LSTM\": (y_pred_lstm.flatten()),\n",
        "        \"Multi-step LSTM\": (y_pred_multi_lstm[-1]),\n",
        "        \"N-Beats\": (y_pred_nbeats.NBEATS.values.flatten()),\n",
        "        \"Autogluon\": (y_pred_autogluon)\n",
        "    }\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "PUEBUcR8rAQS"
      },
      "id": "PUEBUcR8rAQS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O gráfico acima traz todas as previsões geradas pelos modelos para o período de maio/2024 a abril/2025.\n",
        "\n",
        "Por ser interativo, encoraja-se a filtragem dos resultados para comparação dos resultados entre modelos específicos."
      ],
      "metadata": {
        "id": "wXQ5knvpC2lY"
      },
      "id": "wXQ5knvpC2lY"
    },
    {
      "cell_type": "markdown",
      "id": "A4JcV4jXYCrE",
      "metadata": {
        "id": "A4JcV4jXYCrE"
      },
      "source": [
        "### Conclusões"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"justify\">\n",
        "\n",
        "A análise comparativa entre diferentes modelos de previsão de séries temporais demonstrou que a escolha da técnica adequada depende fortemente das características dos dados e do objetivo da previsão. Modelos estatísticos tradicionais, como SARIMA e Holt-Winters, apresentaram desempenho satisfatório, superando a abordagem Naïve e evidenciando sua robustez em séries com padrões de tendência e sazonalidade. O Prophet, por sua vez, obteve resultados intermediários, confirmando sua utilidade prática pela facilidade de ajuste e interpretação, ainda que com precisão inferior aos modelos mais sofisticados.\n",
        "\n",
        "Entre os métodos de aprendizado de máquina, o XGBoost Regressor destacou-se como a melhor solução, alcançando os menores erros e o maior coeficiente de determinação (R²), evidenciando sua capacidade de capturar relações não lineares complexas e superar tanto os modelos estatísticos quanto os de deep learning testados. Já as arquiteturas de redes neurais, como Multi-Step LSTM e N-BEATS, não apresentaram desempenho competitivo neste cenário, sugerindo que sua eficácia pode depender de conjuntos de dados mais extensos, de uma engenharia de atributos mais elaborada ou de ajustes mais profundos de hiperparâmetros. O Multi-step LSTM, N-Beats e o AutoGluon obtiveram resultados particularmente insatisfatórios, indicando que, para esta aplicação, suas abordagens não se mostraram adequadas.\n",
        "\n",
        "Dessa forma, conclui-se que, no contexto analisado, a utilização de algoritmos baseados em gradient boosting, representados pelo XGBoost, é a estratégia mais eficiente para previsão de gastos com alimentação em organizações da Marinha. Os resultados reforçam a importância de uma avaliação empírica cuidadosa e de comparações sistemáticas entre diferentes classes de modelos, uma vez que a sofisticação do método não garante, por si só, melhor desempenho. Trabalhos futuros podem explorar estratégias híbridas, combinações de modelos e o uso de variáveis exógenas para aprimorar ainda mais a acurácia das previsões."
      ],
      "metadata": {
        "id": "vLGzeA16D0DX"
      },
      "id": "vLGzeA16D0DX"
    },
    {
      "cell_type": "markdown",
      "id": "elR7bye6YFZn",
      "metadata": {
        "id": "elR7bye6YFZn"
      },
      "source": [
        "## Trabalhos futuros"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"justify\">\n",
        "\n",
        "Como proposta de continuidade deste estudo, pretende-se avançar além da etapa de previsão de custos e direcionar esforços para o planejamento de ações efetivas voltadas à redução dos gastos com alimentação.\n",
        "Como já vimos, os gastos com alimentação são compostos pelos custos de aquisição dos alimentos distribuídos pelo cardápio multiplicados pela quantidade de pessoas que se alimentarão.\n",
        "Visando reduzir o custo da alimentação sem reduzir nem a qualidade e nem a quatidade de alimentação ofertada, nos resta reduzir os custos para aquisição, que na sua maioria se dá através de licitações.\n",
        "Para tanto, sugere-se o desenvolvimento de um modelo de clusterização das organizações militares, utilizando uma função de distância personalizada que considere as distâncias rodoviárias e hidroviárias, associadas aos respectivos custos logísticos. Essa abordagem permitirá identificar agrupamentos naturais de unidades e, a partir deles, propor a criação de centros de distribuição em locais estratégicos, capazes de otimizar o fornecimento de gêneros alimentícios.\n",
        "\n",
        "A centralização das aquisições nesses polos logísticos pode potencializar o ganho de escala, viabilizando negociações diretas com distribuidores ou produtores, resultando em maior poder de barganha, redução de custos unitários e racionalização da cadeia de suprimentos. Além disso, a metodologia proposta abre caminho para a incorporação de técnicas de otimização combinatória e de simulação de cenários, que poderão subsidiar decisões estratégicas sobre a localização e a quantidade ideal de centros de distribuição, equilibrando custos logísticos e operacionais. Dessa forma, o possível próximo trabalho buscará não apenas prever, mas também intervir no processo de gestão de suprimentos, oferecendo recomendações práticas de alto impacto para a eficiência administrativa e a sustentabilidade financeira da Marinha."
      ],
      "metadata": {
        "id": "4fiZQeLez0m4"
      },
      "id": "4fiZQeLez0m4"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualização das localizações das OM\n",
        "#@markdown Abaixo podemos ver a complexidade logística de abastecimento de organizações que se encontram em todo o território brasileiro, com sua proporção continetal.\n",
        "#@markdown <br>As linhas negras do gráfico são a malha rodoviária, enquanto as hidrovias são representadas pelas linhas azuis.\n",
        "#@markdown\n",
        "\n",
        "# Importa a biblioteca GeoPandas, usada para manipulação de dados espaciais (geometrias, shapefiles, etc.)\n",
        "import geopandas as gpd\n",
        "\n",
        "# Importa a biblioteca Folium, usada para criar mapas interativos em Python\n",
        "import folium\n",
        "\n",
        "# Importa o recurso MarkerCluster do Folium, que agrupa marcadores próximos em um mesmo ponto do mapa\n",
        "# Isso ajuda a visualizar melhor os dados em regiões com muitos pontos\n",
        "from folium.plugins import MarkerCluster\n",
        "\n",
        "# Carrega a tabela de endereços das organizações (com colunas como latitude, longitude e nome da OM)\n",
        "enderecos = pd.read_csv('/content/MVP/dados/logistica/enderecos.csv')\n",
        "\n",
        "# Carrega o shapefile das rodovias do Brasil (linhas representando estradas)\n",
        "rodovias = gpd.read_file('/content/MVP/dados/logistica/rodovias.shp')\n",
        "\n",
        "# Carrega o shapefile das hidrovias do Brasil (linhas representando rotas navegáveis)\n",
        "hidrovias = gpd.read_file('/content/MVP/dados/logistica/hidrovias.shp')\n",
        "\n",
        "# Cria o mapa base centralizado aproximadamente no centro geográfico do Brasil\n",
        "# tiles='openstreetmap' define o estilo do mapa, e zoom_start=4 ajusta o nível inicial de zoom\n",
        "brasil = folium.Map(location=[-11.793229, -52.991287], tiles='openstreetmap', zoom_start=4)\n",
        "\n",
        "# Adiciona as rodovias no mapa como linhas pretas finas\n",
        "folium.Choropleth(\n",
        "    rodovias,\n",
        "    line_weight=0.5,\n",
        "    line_color='black'\n",
        ").add_to(brasil)\n",
        "\n",
        "# Adiciona as hidrovias no mapa como linhas azuis mais destacadas\n",
        "folium.Choropleth(\n",
        "    hidrovias,\n",
        "    line_weight=1,\n",
        "    line_color='blue'\n",
        ").add_to(brasil)\n",
        "\n",
        "# Cria um agrupador de marcadores (para evitar poluição visual quando vários pontos estão próximos)\n",
        "marker_cluster = MarkerCluster()\n",
        "\n",
        "# Adiciona um marcador para cada endereço válido (sem valores ausentes)\n",
        "# Cada marcador mostra a latitude e longitude, e no popup aparece o nome da Organização Militar (OM)\n",
        "for i, row in enderecos.dropna(how='any').iterrows():\n",
        "    folium.Marker([row.latitude, row.longitude], popup=row.OM).add_to(marker_cluster)\n",
        "\n",
        "# Insere o agrupador de marcadores no mapa\n",
        "marker_cluster.add_to(brasil)\n",
        "\n",
        "# Exibe o mapa interativo final com rodovias, hidrovias e as localizações das OMs\n",
        "brasil\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cjfVu4Y3WmGl"
      },
      "id": "cjfVu4Y3WmGl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"justify\">\n",
        "\n",
        "Uma clusterização para redução dos custos levaria em consideração variáveis como o custo da modalidade de transporte, o custo de manutenção dos centros de distribuição e a quantidade de pessoas a serem atendidas por estes centros, para verificar se a economia de escala justificaria a criação.\n",
        "\n",
        "Além de uma economia, o resultado traria mais eficiência para a administração ao centralizar estas aquisições evitando a pulverização da carga administrativa."
      ],
      "metadata": {
        "id": "23gPFcIF6hhn"
      },
      "id": "23gPFcIF6hhn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"justify\">\n",
        "\n",
        "## Agradecimentos\n",
        "\n",
        "Ao concluir este trabalho, registro minha sincera gratidão a equipe de professores e aos orientadores da MB que,  de forma direta ou indireta, contribuíram para o meu aprendizado ao longo desta jornada. Cada desafio enfrentado e cada conhecimento adquirido reforçam em mim a convicção de que o estudo e a pesquisa são caminhos contínuos e transformadores.\n",
        "\n",
        "Sou profundamente grato pelos ensinamentos recebidos, que não apenas possibilitaram a realização deste trabalho, mas também despertam em mim a vontade de seguir explorando novas ideias, metodologias e aplicações práticas. Reconheço que cada passo dado até aqui foi apenas o início de um processo de constante evolução, e encaro o futuro com entusiasmo para aprender mais, aperfeiçoar o que já foi desenvolvido e gerar resultados cada vez mais significativos."
      ],
      "metadata": {
        "id": "DxqJXiSOBEYh"
      },
      "id": "DxqJXiSOBEYh"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}