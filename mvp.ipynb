{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "269fe9a4",
      "metadata": {
        "id": "269fe9a4"
      },
      "source": [
        "[![Abra no Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ffserro/MVP/blob/master/mvp.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wL7a42D2_9De",
      "metadata": {
        "id": "wL7a42D2_9De"
      },
      "source": [
        "# Regressão Linear para Series Temporais - Planejamento dos dispêndios de alimentação de militares da Marinha do Brasil"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jkWGTW6eI5k9",
      "metadata": {
        "id": "jkWGTW6eI5k9"
      },
      "source": [
        "## Contextualização\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JgQOsnRyiCbK",
      "metadata": {
        "id": "JgQOsnRyiCbK"
      },
      "source": [
        "### O que é o municiamento?\n",
        "A Gestoria de Munciamento é o conjunto dos processos responsáveis por gerir diariamente a alimentação e subsistência dos militares e servidores lotados nas organizações militares da Marinha do Brasil.\n",
        "\n",
        "As principais atividades da gestoria de municiamento são:\n",
        "- Planejamento e aquisição de gêneros alimentícios\n",
        "- Controle de estoque de gêneros alimentícios\n",
        "- Escrituração e pagamento\n",
        "- Prestação de contas\n",
        "\n",
        "\n",
        "### Orçamento público e alimentação de militares\n",
        "Por ser custeada com recursos do orçamento da União, a gestoria de municiamento se submete a um sistema rigoroso de planejamento, controle, fiscalização e transparência, para garantir que os valores sejam utilizados de forma eficiente, econômica e legal.\n",
        "\n",
        "A compra de gêneros alimentícios é uma despesa recorrente e significativa. Uma gestão eficiente garante que os recursos financeiros sejam alocados de forma inteligente, evitando gastos desnecessários.\n",
        "\n",
        "Uma boa gestão de estoques minimiza desperdícios de alimentos por validade ou má conservação.\n",
        "\n",
        "Como em qualquer gasto público, a aquisição de suprimentos deve ser transparente e seguir todas as normas de controle, visando a economia e a responsabilidade fiscal."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9T92loE8Bgsx",
      "metadata": {
        "id": "9T92loE8Bgsx"
      },
      "source": [
        "<div align=\"justify\">\n",
        "O planejamento eficiente dos recursos logísticos é um dos pilares para a manutenção da prontidão e da capacidade operacional das Forças Armadas. Entre os diversos insumos estratégicos, a alimentação das organizações militares desempenha papel central, tanto no aspecto orçamentário quanto no suporte direto às atividades diárias. Na Marinha do Brasil, a gestão dos estoques e dos gastos com gêneros alimentícios envolve múltiplos órgãos e abrange um volume expressivo de transações financeiras e contábeis, tornando-se um processo complexo e suscetível a variações sazonais, econômicas e administrativas.\n",
        "\n",
        "Neste cenário, prever com maior precisão os custos relacionados ao consumo de alimentos é fundamental para otimizar a alocação de recursos públicos, reduzir desperdícios, evitar rupturas de estoque e aumentar a eficiência do planejamento orçamentário. Tradicionalmente, esse processo é conduzido por meio de análises históricas e técnicas de planejamento administrativo. No entanto, tais abordagens muitas vezes não capturam adequadamente os padrões temporais e as variáveis externas que influenciam os gastos.\n",
        "\n",
        "A ciência de dados, e em particular as técnicas de modelagem de séries temporais, surge como uma alternativa poderosa para aprimorar esse processo decisório. Modelos como SARIMA, Prophet, XGBoost e LSTM permitem identificar tendências, sazonalidades e anomalias nos dados, possibilitando não apenas previsões mais robustas, mas também a geração de insights que subsidiam políticas de abastecimento e aquisição.\n",
        "\n",
        "Assim, o presente trabalho propõe a aplicação de técnicas de análise e previsão de séries temporais sobre os dados históricos de consumo de alimentos da Marinha do Brasil, com o objetivo de estimar os custos futuros e explorar padrões relevantes que possam apoiar o processo de gestão logística e orçamentária. A relevância deste estudo reside não apenas no ganho potencial de eficiência administrativa, mas também na contribuição para a transparência, a racionalização do gasto público e a modernização da gestão de suprimentos em instituições estratégicas para o país.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CcOtcOjHAdVT",
      "metadata": {
        "id": "CcOtcOjHAdVT"
      },
      "source": [
        "## Glossário\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VlDqciiHAizg",
      "metadata": {
        "id": "VlDqciiHAizg"
      },
      "source": [
        "* Municiamento\n",
        "* Rancho\n",
        "* Etapa\n",
        "* Comensal\n",
        "* Série Temporal\n",
        "* Tendência\n",
        "* Sazonalidade\n",
        "* Estacionariedade\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EDC39fUr3nJB",
      "metadata": {
        "id": "EDC39fUr3nJB"
      },
      "source": [
        "## Modelagem"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N_AvpbxG3uIJ",
      "metadata": {
        "id": "N_AvpbxG3uIJ"
      },
      "source": [
        "<div align=\"justify\">\n",
        " O conjunto de dados que será apresentado traz informações sobre despesas mensais com alimentação de militares e servidores de grandes organizações.\n",
        "\n",
        " O balanço de paiol do mês anterior nos traz a informação valor total dos gêneros alimentícios armazenados na organização no último dia do mês anterior.\n",
        "\n",
        " Os gêneros podem ser adquiridos pelas organizações de quatro formas diferentes:\n",
        " - adquirindo os gêneros dos depósitos de subsistência da Marinha\n",
        " - adquirindo os gêneros através de listas de fornecimento de gêneros, que são licitações centralizadas realizadas para atender toda a Marinha\n",
        " - adquirindo os gêneros através da realização de licitações próprias\n",
        " - adquirindo os gêneros através de contratação direta, sem licitação\n",
        "\n",
        " As organizações podem transferir gêneros entre seus estoques, através da realização de remessas. Os gêneros são contabilizados então no paiol através de remessas recebidas e remesas expedidas.\n",
        "\n",
        " Os gêneros consumidos durante as refeições do dia (café da manhã, almoço, janta e ceia) são contabilizados como gêneros consumidos.\n",
        "\n",
        " Os gêneros consumidos fora das refeições, como o biscoito, café e açúcar que são consumidos durante o dia, são contabilizados como vales-extra.\n",
        "\n",
        " As eventuais perdas de estoque são contabilizadas como termos de despesa.\n",
        "\n",
        " Quanto às receitas, cada comensal lotado na organização autoriza um determinado valor despesa por dia. A soma dessa despesa autorizada no mês é o valor limite dos gêneros que poderão ser retirados do paiol.\n",
        "\n",
        " A modelagem para os dispêndios com gêneros alimentícios considera as seguintes variáveis:\n",
        " - a quantidade de pessoas às quais é oferecida alimentação\n",
        " - o custo dos alimentos em paiol\n",
        " - a composição do cardápio (englobando o perfil de consumo de cada organização)\n",
        "\n",
        " Assim, o gasto mensal $Y_m$ de determinada organização no mês $m$ pode ser expresso em termos de:\n",
        " - Efetivo atendido ($N_m$)\n",
        " - Custo de aquisição dos insumos ($P_m$)\n",
        " - Composição do cardápio e perfil de consumo ($C_m$)\n",
        "\n",
        " Ou seja, $Y_m = f(N_m, P_m, C_m)\\ +\\ ϵ_m$\n",
        "\n",
        " sendo que o gasto mensal $Y_m$ é o somatório dos gastos diários $Y_d$, expressos por:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{Y_d} = \\sum_{i=1}^q \\ N_i \\cdot p_i \\\\ \\\\ \\mathbf{Y_m} = \\sum_{i=1}^{30} Y_{di}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3sDv8fXfJeYN",
      "metadata": {
        "id": "3sDv8fXfJeYN"
      },
      "source": [
        "## Trabalho"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IcDCsWxvPIxI",
      "metadata": {
        "id": "IcDCsWxvPIxI"
      },
      "outputs": [],
      "source": [
        "#@title Downloads necessários\n",
        "\n",
        "# SE é a primeira vez que esta célula está sendo executada na sessão ENTÃO baixe os arquivos hospedados no github E instale as dependências do projeto.\n",
        "![ ! -f '/content/pip_log.txt' ] && git clone 'https://github.com/ffserro/MVP.git' && pip uninstall torchvision torch torchaudio -y > '/content/pip_log.txt' && pip install -r '/content/MVP/requirements.txt' > '/content/pip_log.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acb69f76",
      "metadata": {
        "id": "acb69f76"
      },
      "outputs": [],
      "source": [
        "#@title Import de bibliotecas\n",
        "\n",
        "# ============================================================\n",
        "# Utilitários do sistema e manipulação de arquivos/datas\n",
        "# ============================================================\n",
        "from glob import glob                                         # Seleção de múltiplos arquivos via padrões (ex.: *.csv)\n",
        "from datetime import datetime as dt, timedelta as td          # Manipulação de datas e intervalos de tempo\n",
        "import matplotlib.dates as mdates                             # Manipulação de datas dos eixos dos gráficos\n",
        "import itertools                                              # Criação de combinações e iterações eficientes\n",
        "\n",
        "# ============================================================\n",
        "# Manipulação numérica e de dados\n",
        "# ============================================================\n",
        "import pandas as pd                                           # Estruturas de dados tabulares (DataFrames)\n",
        "import numpy as np                                            # Computação numérica de alta performance (arrays/vetores)\n",
        "\n",
        "# ============================================================\n",
        "# Visualização de dados\n",
        "# ============================================================\n",
        "import matplotlib.pyplot as plt                               # Visualizações estáticas básicas (gráficos 2D)\n",
        "import seaborn as sns                                         # Visualizações estatísticas de alto nível (heatmaps, distribuições)\n",
        "import plotly.express as px                                   # Visualizações interativas de alto nível\n",
        "import plotly.graph_objects as go                             # Visualizações interativas detalhadas e customizáveis\n",
        "\n",
        "# ============================================================\n",
        "# Ferramentas estatísticas para séries temporais\n",
        "# ============================================================\n",
        "import statsmodels.api as sm                                  # Modelos estatísticos gerais\n",
        "from statsmodels.tsa.stattools import adfuller                # Teste ADF (estacionariedade)\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose       # Decomposição de série (tendência/sazonalidade)\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf # Autocorrelação e autocorrelação parcial\n",
        "\n",
        "# ============================================================\n",
        "# Modelos clássicos de séries temporais (baseline)\n",
        "# ============================================================\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX        # Modelo SARIMA/SARIMAX\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing  # Suavização exponencial de Holt-Winters\n",
        "\n",
        "# ============================================================\n",
        "# Preparação e avaliação de dados\n",
        "# ============================================================\n",
        "from sklearn.model_selection import TimeSeriesSplit           # Validação cruzada para séries temporais\n",
        "from sklearn.preprocessing import MinMaxScaler                # Normalização (0–1) para redes neurais e ML\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error  # Métricas de avaliação (MAE, MSE)\n",
        "\n",
        "# ============================================================\n",
        "# Modelos modernos de previsão\n",
        "# ============================================================\n",
        "from prophet import Prophet                                   # Modelo Prophet (captura tendência + sazonalidade)\n",
        "from xgboost import XGBRegressor                              # Modelo baseado em boosting (árvores de decisão)\n",
        "\n",
        "# Redes neurais (TensorFlow/Keras)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential                # Estrutura sequencial de camadas\n",
        "from tensorflow.keras.layers import LSTM, Dense,\\\n",
        " BatchNormalization # Camadas para deep learning\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau # Callbacks para treinamento robusto\n",
        "\n",
        "# Modelos baseados em deep learning específicos de séries temporais\n",
        "from neuralforecast import NeuralForecast\n",
        "from neuralforecast.models import NBEATS                      # Arquitetura N-BEATS (estado da arte em forecasting)\n",
        "\n",
        "# AutoML para séries temporais\n",
        "from autogluon.timeseries import TimeSeriesPredictor          # AutoGluon (seleção automática de modelos)\n",
        "\n",
        "# ============================================================\n",
        "# Configurações gerais\n",
        "# ============================================================\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')                                      # Silencia warnings para manter a saída do notebook limpa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5b106c1",
      "metadata": {
        "id": "c5b106c1"
      },
      "outputs": [],
      "source": [
        "#@title Leitura dos dados brutos\n",
        "\n",
        "# ============================================================\n",
        "# Leitura e consolidação dos dados brutos\n",
        "# ============================================================\n",
        "\n",
        "# Carrega todos os arquivos de movimentação mensal (mmm) em um único DataFrame.\n",
        "# - parse_dates=[['ano', 'mes']] combina as colunas 'ano' e 'mes' em um único campo datetime\n",
        "# - glob encontra todos os arquivos .xlsx dentro da pasta de dados\n",
        "mmm = pd.DataFrame()\n",
        "mmm = pd.concat(\n",
        "    [mmm] + [\n",
        "        pd.read_excel(arquivo, parse_dates=[['ano', 'mes']])\n",
        "        for arquivo in glob('/content/MVP/dados/mmm/*.xlsx')\n",
        "    ],\n",
        "    ignore_index=True\n",
        ")\n",
        "\n",
        "# Carrega todos os arquivos de etapas em um único DataFrame.\n",
        "# Estrutura e lógica de leitura iguais ao dataset acima\n",
        "etapas = pd.DataFrame()\n",
        "etapas = pd.concat(\n",
        "    [etapas] + [\n",
        "        pd.read_excel(arquivo, parse_dates=[['ano', 'mes']])\n",
        "        for arquivo in glob('/content/MVP/dados/etapas/*.xlsx')\n",
        "    ],\n",
        "    ignore_index=True\n",
        ")\n",
        "\n",
        "# Carrega as informações das organizações militares centralizadas e suas respectivas centralizadoras\n",
        "centralizadas = pd.read_csv('/content/MVP/dados/om_centralizada.csv')\n",
        "\n",
        "# Carrega informações complementares sobre as organizações militares\n",
        "om_info = pd.read_csv('/content/MVP/dados/om_info.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D2taCARVZEwt",
      "metadata": {
        "id": "D2taCARVZEwt"
      },
      "source": [
        "### Limpeza dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Un4l_KcXaoz6",
      "metadata": {
        "id": "Un4l_KcXaoz6"
      },
      "source": [
        "#### Limpeza do conjunto de dados sobre os Mapas Mensais do Municiamento de 2019 a 2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i7JCu7ZWalzw",
      "metadata": {
        "id": "i7JCu7ZWalzw"
      },
      "outputs": [],
      "source": [
        "# Existem organizações que não possuem dados para todo o período analisado\n",
        "mmm.groupby('nome').ano_mes.count().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q-s3ueiHa6Lu",
      "metadata": {
        "id": "q-s3ueiHa6Lu"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Ao todo o conjunto de dados contempla 80 meses. Como será realizada uma análise de série temporal, vou considerar apenas as organizações que possuem dados para todo o período.\n",
        "mmm = mmm[mmm.codigo.isin(mmm.codigo.value_counts()[mmm.codigo.value_counts() == 80].index)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4_xJLDpda_bB",
      "metadata": {
        "id": "4_xJLDpda_bB"
      },
      "outputs": [],
      "source": [
        "mmm = mmm[mmm.ano_mes < dt(2025, 7, 1)] # removendo os dados dos últimos meses. Não são confiáveis porque as comprovações ainda não tinham sido completamente consolidadas a época da coleta de dados."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WMzcvFijbCoc",
      "metadata": {
        "id": "WMzcvFijbCoc"
      },
      "source": [
        "#### Limpeza do conjunto de dados sobre informações das OM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J0gpOj7abIzH",
      "metadata": {
        "id": "J0gpOj7abIzH"
      },
      "outputs": [],
      "source": [
        "# Verificando a existência de dados faltosos\n",
        "(om_info.isna().sum()/len(om_info)).sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WEaLd60hbRP_",
      "metadata": {
        "id": "WEaLd60hbRP_"
      },
      "outputs": [],
      "source": [
        "# Quantidade de entradas diferentes para cada coluna no conjunto de dados\n",
        "om_info.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_ZGPVcopbPZg",
      "metadata": {
        "id": "_ZGPVcopbPZg"
      },
      "outputs": [],
      "source": [
        "# Informações que não vao agregar conhecimento para o caso em tela, por serem nulos ou por conter informações irrelevantes\n",
        "om_info.drop(columns=['COMIMSUP', 'CNPJ', 'TELEFONE', 'ODS', 'TIPO_CONEXAO', 'CRIACAO', 'MODIFICACAO'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MsnEeL4abTYP",
      "metadata": {
        "id": "MsnEeL4abTYP"
      },
      "outputs": [],
      "source": [
        "om_info[['DN_ID', 'SUB_DN_ID', 'AREA_ID', 'COD_SQ_LOCAL', 'NOME', 'CIDADE']].sort_values(by=['DN_ID', 'SUB_DN_ID', 'AREA_ID', 'COD_SQ_LOCAL'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Yg0iX83rbU5h",
      "metadata": {
        "id": "Yg0iX83rbU5h"
      },
      "outputs": [],
      "source": [
        "# As colunas SUB_DN_ID, AREA_ID e COD_SQ_LOCAL são pouco ou não descritivas\n",
        "om_info.drop(columns=['SUB_DN_ID', 'AREA_ID', 'COD_SQ_LOCAL'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iItk2qWRbV-N",
      "metadata": {
        "id": "iItk2qWRbV-N"
      },
      "outputs": [],
      "source": [
        "om_info.sort_values(by='TIPO')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ibe9IhfVbX00",
      "metadata": {
        "id": "Ibe9IhfVbX00"
      },
      "outputs": [],
      "source": [
        "# A variável TIPO começa descrevendo os tipo de organizações, como A para bases aeronavais, B para bases, F para fuzileiros navais, N para navios, S para saúde e I para instrução. Porém o T entra em uma categoria geral como se em algum momento essa vaiável deixou de ser utilizada.\n",
        "# Então se tornou pouco descritiva para os nossos objetivos\n",
        "\n",
        "om_info.drop(columns=['TIPO'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qFC_VhAGbY9P",
      "metadata": {
        "id": "qFC_VhAGbY9P"
      },
      "outputs": [],
      "source": [
        "# observações com dados faltosos\n",
        "om_info.loc[om_info.isna().sum(axis=1) != 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qDD21OiibaRy",
      "metadata": {
        "id": "qDD21OiibaRy"
      },
      "outputs": [],
      "source": [
        "# Preenchendo manualmente os dados faltosos com informações da internet\n",
        "\n",
        "om_info.loc[om_info.CODIGO==87310, 'BAIRRO'] = 'Plano Diretor Sul'\n",
        "om_info.loc[om_info.CODIGO==87700, 'BAIRRO'] = 'Asa Sul'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IBy35wKwbcIT",
      "metadata": {
        "id": "IBy35wKwbcIT"
      },
      "source": [
        "#### Limpeza de dados do conjunto de dados sobre centralização do municiamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HBIVBMedbiqR",
      "metadata": {
        "id": "HBIVBMedbiqR"
      },
      "outputs": [],
      "source": [
        "# Primeira transformação a ser feita será padronizar a codificação das organizações por UASG\n",
        "\n",
        "centralizadas['OM_CENTRALIZADA_ID'] = centralizadas.OM_CENTRALIZADA_ID.map(om_info.set_index('ID').CODIGO)\n",
        "centralizadas['OM_CENTRALIZADORA_ID'] = centralizadas.OM_CENTRALIZADORA_ID.map(om_info.set_index('ID').CODIGO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R6UyXDrDbmGD",
      "metadata": {
        "id": "R6UyXDrDbmGD"
      },
      "outputs": [],
      "source": [
        "# Mais uma vez, eu só preciso das informações das organizações que estão presentes no conjunto de dados do Mapa Mensal do Municiamento\n",
        "centralizadas = centralizadas[centralizadas.OM_CENTRALIZADORA_ID.isin(mmm.codigo.unique())]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xCN5KQq8bnaJ",
      "metadata": {
        "id": "xCN5KQq8bnaJ"
      },
      "outputs": [],
      "source": [
        "# Drop de colunas pouco informativas para o problema em tela\n",
        "\n",
        "centralizadas.drop(columns=['CONTATO', 'TELEFONE', 'CRIACAO', 'MODIFICACAO', 'GESTORIA_ID'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P4YlPXPzbowk",
      "metadata": {
        "id": "P4YlPXPzbowk"
      },
      "outputs": [],
      "source": [
        "# Remover do conjunto de dados as movimentações que aconteceram antes do período observado\n",
        "centralizadas = centralizadas[~(pd.to_datetime(centralizadas.DATA_FIM) < dt(2019, 1, 1))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LMybqLoNbqkc",
      "metadata": {
        "id": "LMybqLoNbqkc"
      },
      "outputs": [],
      "source": [
        "# Verificação manual da coerência dos períodos municiados\n",
        "\n",
        "centralizadas.groupby('OM_CENTRALIZADA_ID').filter(lambda x: len(x)> 1).sort_values(by=['OM_CENTRALIZADA_ID', 'DATA_INICIO'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XKRjKiMCbuBP",
      "metadata": {
        "id": "XKRjKiMCbuBP"
      },
      "outputs": [],
      "source": [
        "# Definindo algumas datas que as organizações passaram a ser centralizadas por outra centralizadora\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==11500) & (centralizadas.OM_CENTRALIZADORA_ID==71000), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==11500) & (centralizadas.OM_CENTRALIZADORA_ID==81000), 'DATA_INICIO']\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==49000) & (centralizadas.OM_CENTRALIZADORA_ID==71000), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==49000) & (centralizadas.OM_CENTRALIZADORA_ID==81000), 'DATA_INICIO']\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==62000) & (centralizadas.OM_CENTRALIZADORA_ID==62000), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==62000) & (centralizadas.OM_CENTRALIZADORA_ID==81000), 'DATA_INICIO']\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==62500) & (centralizadas.OM_CENTRALIZADORA_ID==62500), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==62500) & (centralizadas.OM_CENTRALIZADORA_ID==81000), 'DATA_INICIO']\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==64000) & (centralizadas.OM_CENTRALIZADORA_ID==64000), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==64000) & (centralizadas.OM_CENTRALIZADORA_ID==81000), 'DATA_INICIO']\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==65701) & (centralizadas.OM_CENTRALIZADORA_ID==65701), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==65701) & (centralizadas.OM_CENTRALIZADORA_ID==81000), 'DATA_INICIO']\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==65730) & (centralizadas.OM_CENTRALIZADORA_ID==65701), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==65730) & (centralizadas.OM_CENTRALIZADORA_ID==81000), 'DATA_INICIO']\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==67000) & (centralizadas.OM_CENTRALIZADORA_ID==62000), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==67000) & (centralizadas.OM_CENTRALIZADORA_ID==81000), 'DATA_INICIO']\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==71000) & (centralizadas.OM_CENTRALIZADORA_ID==71000), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==71000) & (centralizadas.OM_CENTRALIZADORA_ID==71100), 'DATA_INICIO']\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==71000) & (centralizadas.OM_CENTRALIZADORA_ID==71100), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==71000) & (centralizadas.OM_CENTRALIZADORA_ID==81000), 'DATA_INICIO']\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==72000) & (centralizadas.OM_CENTRALIZADORA_ID==71000), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==72000) & (centralizadas.OM_CENTRALIZADORA_ID==81000), 'DATA_INICIO']\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==73000) & (centralizadas.OM_CENTRALIZADORA_ID==71000), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==73000) & (centralizadas.OM_CENTRALIZADORA_ID==81000), 'DATA_INICIO']\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==73200) & (centralizadas.OM_CENTRALIZADORA_ID==71000), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==73200) & (centralizadas.OM_CENTRALIZADORA_ID==81000), 'DATA_INICIO']\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==76000) & (centralizadas.OM_CENTRALIZADORA_ID==71000), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==76000) & (centralizadas.OM_CENTRALIZADORA_ID==81000), 'DATA_INICIO']\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==78000) & (centralizadas.OM_CENTRALIZADORA_ID==81000), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==78000) & (centralizadas.OM_CENTRALIZADORA_ID==71000), 'DATA_INICIO']\n",
        "centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==80000) & (centralizadas.OM_CENTRALIZADORA_ID==80000), 'DATA_FIM'] = centralizadas.loc[(centralizadas.OM_CENTRALIZADA_ID==80000) & (centralizadas.OM_CENTRALIZADORA_ID==81000), 'DATA_INICIO']\n",
        "\n",
        "# Removendo algumas informações que estavam duplicadas\n",
        "centralizadas.drop(index=centralizadas[(centralizadas.OM_CENTRALIZADA_ID==62600) & (centralizadas.OM_CENTRALIZADORA_ID==62600) & (centralizadas.TIPO_CENTRALIZACAO_ID.isna())].index, inplace=True)\n",
        "centralizadas.drop(index=centralizadas[(centralizadas.OM_CENTRALIZADA_ID==87400) & (centralizadas.OM_CENTRALIZADORA_ID==87400) & (centralizadas.TIPO_CENTRALIZACAO_ID.isna())].index, inplace=True)\n",
        "centralizadas.drop(index=centralizadas[(centralizadas.OM_CENTRALIZADA_ID==88000) & (centralizadas.OM_CENTRALIZADORA_ID==88000) & (centralizadas.TIPO_CENTRALIZACAO_ID.isna())].index, inplace=True)\n",
        "centralizadas.drop(index=centralizadas[(centralizadas.OM_CENTRALIZADA_ID==88133) & (centralizadas.OM_CENTRALIZADORA_ID==88133) & (centralizadas.TIPO_CENTRALIZACAO_ID.isna())].index, inplace=True)\n",
        "centralizadas.drop(index=centralizadas[(centralizadas.OM_CENTRALIZADA_ID==88701) & (centralizadas.OM_CENTRALIZADORA_ID==88000) & (centralizadas.DATA_FIM.isna())].index, inplace=True)\n",
        "centralizadas.drop(index=centralizadas[(centralizadas.OM_CENTRALIZADA_ID==95300) & (centralizadas.OM_CENTRALIZADORA_ID==95380) & (centralizadas.TIPO_CENTRALIZACAO_ID.isna())].index, inplace=True)\n",
        "centralizadas.drop(index=centralizadas[(centralizadas.OM_CENTRALIZADA_ID==95340) & (centralizadas.OM_CENTRALIZADORA_ID==95380) & (centralizadas.TIPO_CENTRALIZACAO_ID.isna())].index, inplace=True)\n",
        "centralizadas.drop(index=centralizadas[(centralizadas.OM_CENTRALIZADA_ID==95370) & (centralizadas.OM_CENTRALIZADORA_ID==95380) & (centralizadas.TIPO_CENTRALIZACAO_ID.isna())].index, inplace=True)\n",
        "centralizadas.drop(index=centralizadas[(centralizadas.OM_CENTRALIZADA_ID==95380) & (centralizadas.OM_CENTRALIZADORA_ID==95380) & (centralizadas.DATA_INICIO==dt(2004, 1, 1))].index, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "klA8qECFbvRI",
      "metadata": {
        "id": "klA8qECFbvRI"
      },
      "outputs": [],
      "source": [
        "# Supondo que as relações que não possuem data fim estão em vigor até hoje\n",
        "centralizadas.DATA_FIM.fillna(dt(2026,1,1), inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4kc_m-hDbwnc",
      "metadata": {
        "id": "4kc_m-hDbwnc"
      },
      "source": [
        "#### Limpeza do conjunto de dados sobre etapas do municiamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8eTK5TjYb0xh",
      "metadata": {
        "id": "8eTK5TjYb0xh"
      },
      "outputs": [],
      "source": [
        "# Filtro para manter apenas etapas que sejam relevantes dado as organizações contantes do conjunto de dados dos Mapas Mensais do Municiamento\n",
        "etapas = etapas[etapas.uasg.isin(mmm.codigo.unique())]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nz_1qH87b23h",
      "metadata": {
        "id": "nz_1qH87b23h"
      },
      "outputs": [],
      "source": [
        "# Removendo as etapas de complementos, uma vez que o objetivo da contabilização das etapas é contar o número de pessoas de cada organização\n",
        "etapas = etapas[~(etapas.codigo_etapa//100==6)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pNsMWpnjcAfp",
      "metadata": {
        "id": "pNsMWpnjcAfp"
      },
      "source": [
        "#### Salvando os dados limpos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iaJ1PP35b_me",
      "metadata": {
        "id": "iaJ1PP35b_me"
      },
      "outputs": [],
      "source": [
        "mmm.to_csv('/content/MVP/dados/mmm/mmm_limpo.csv', index=False)\n",
        "om_info.to_csv('/content/MVP/dados/om_info_limpo.csv', index=False)\n",
        "centralizadas.to_csv('/content/MVP/dados/om_centralizada_limpo.csv', index=False)\n",
        "etapas.to_csv('/content/MVP/dados/etapas/etapas_limpo.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sbYdXOLiEqAY",
      "metadata": {
        "id": "sbYdXOLiEqAY"
      },
      "source": [
        "### Exploração"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d23b319b",
      "metadata": {
        "id": "d23b319b"
      },
      "outputs": [],
      "source": [
        "mmm_marinha = mmm.groupby(['ano_mes'])[['totais_balanco_paiol_despesa']].sum().reset_index().rename(columns={'totais_balanco_paiol_despesa':'consumo'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81640c41",
      "metadata": {
        "id": "81640c41"
      },
      "outputs": [],
      "source": [
        "mmm_marinha = mmm_marinha.iloc[:-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jVCau_ZSeEzU",
      "metadata": {
        "id": "jVCau_ZSeEzU"
      },
      "outputs": [],
      "source": [
        "mmm_marinha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f51d785",
      "metadata": {
        "id": "0f51d785"
      },
      "outputs": [],
      "source": [
        "mmm_etapas = pd.merge(left=mmm, right=etapas, how='inner', left_on=['ano_mes', 'codigo'], right_on=['ano_mes', 'uasg'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fe04974",
      "metadata": {
        "id": "4fe04974"
      },
      "outputs": [],
      "source": [
        "# Filtra o dataframe mmm_etapas para incluir apenas os códigos de etapa 103 e 105, que representam diferentes tipos de refeições ou etapas de municiamento.\n",
        "# Em seguida, seleciona as colunas 'ano_mes', 'nome', 'codigo_etapa' e 'quantidade' para visualização.\n",
        "mmm_etapas[mmm_etapas.codigo_etapa.isin([103, 105])][['ano_mes', 'nome', 'codigo_etapa', 'quantidade']]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5HTsVvFHEco_",
      "metadata": {
        "id": "5HTsVvFHEco_"
      },
      "source": [
        "### Verificações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "066e46ec",
      "metadata": {
        "id": "066e46ec"
      },
      "outputs": [],
      "source": [
        "def plota_resultados(title, df=mmm_marinha, x_col='ano_mes', y_col='consumo', preds=None, labels=None):\n",
        "    '''\n",
        "    Plota a série temporal original e, opcionalmente, as previsões de um ou mais modelos.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame contendo a série temporal original.\n",
        "        x_col (str): Nome da coluna do eixo x (geralmente a coluna de data).\n",
        "        y_col (str): Nome da coluna do eixo y (a variável a ser plotada).\n",
        "        title (str): Título do gráfico.\n",
        "        preds (dict, optional): Dicionário onde as chaves são os nomes dos modelos\n",
        "                                 e os valores são tuplas (x_pred, y_pred) com as\n",
        "                                 datas e os valores previstos. Defaults to None.\n",
        "        labels (dict, optional): Dicionário para renomear os rótulos dos eixos\n",
        "                                 no gráfico. Defaults to {x_col: 'Período', y_col: 'Valor observado'}.\n",
        "    '''\n",
        "    # Série real\n",
        "    fig = px.line(\n",
        "        df,\n",
        "        x=x_col,\n",
        "        y=y_col,\n",
        "        labels=labels or {x_col: 'Período', y_col: 'Valor observado'},\n",
        "        title=title\n",
        "    )\n",
        "\n",
        "    # Previsões opcionais\n",
        "    if preds:\n",
        "        for model_name, (x_pred, y_pred) in preds.items():\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=x_pred,\n",
        "                y=y_pred,\n",
        "                mode='lines+markers',\n",
        "                name=f'Previsão — {model_name}',\n",
        "                line=dict(width=2, dash='dash')\n",
        "            ))\n",
        "\n",
        "    # Layout padronizado\n",
        "    fig.update_traces(line=dict(width=2))\n",
        "    fig.update_xaxes(tickangle=45)\n",
        "    fig.update_layout(\n",
        "        template='plotly_white',\n",
        "        hovermode='x unified'\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "# Gera o plot da série temporal original dos gastos com alimentação\n",
        "fig = plota_resultados(\n",
        "title='Gastos com alimentação dos últimos cinco anos'\n",
        ")\n",
        "\n",
        "# Exibe o gráfico\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc3e4dff",
      "metadata": {
        "id": "fc3e4dff"
      },
      "outputs": [],
      "source": [
        "def checa_estacionariedade(serie, alpha=0.05):\n",
        "    '''\n",
        "    Executa o Teste de Dickey-Fuller Aumentado (ADF) para verificar\n",
        "    se uma série temporal é estacionária.\n",
        "\n",
        "    Parâmetros\n",
        "    ----------\n",
        "    serie : pd.Series\n",
        "        Série temporal a ser testada.\n",
        "    alpha : float, default=0.05\n",
        "        Nível de significância para o teste de hipótese.\n",
        "\n",
        "    Interpretação\n",
        "    -------------\n",
        "    H0 (hipótese nula): a série possui raiz unitária (não estacionária).\n",
        "    H1 (alternativa): a série é estacionária.\n",
        "    '''\n",
        "\n",
        "    # Remove valores nulos antes do teste\n",
        "    resultado = adfuller(serie.dropna())\n",
        "\n",
        "    # Extrai estatísticas relevantes\n",
        "    estatistica_adf = resultado[0]\n",
        "    p_valor = resultado[1]\n",
        "    num_lags = resultado[2]     # número de defasagens usadas no modelo\n",
        "    num_obs = resultado[3]      # número de observações utilizadas\n",
        "\n",
        "    # Exibe os principais resultados\n",
        "    print(f'Estatística ADF: {estatistica_adf:.4f}')\n",
        "    print(f'p-valor: {p_valor:.4f}')\n",
        "    print(f'Nº lags utilizados: {num_lags}')\n",
        "    print(f'Nº observações: {num_obs}')\n",
        "    print('Valores críticos:', resultado[4])  # níveis de significância (1%, 5%, 10%)\n",
        "\n",
        "    # Avaliação da hipótese nula\n",
        "    if p_valor < alpha:\n",
        "        print('Série estacionária (rejeita H0 de raiz unitária).')\n",
        "    else:\n",
        "        print('Série não estacionária (não rejeita H0).')\n",
        "\n",
        "# ============================================================\n",
        "# Verificação da estacionariedade da série de consumo\n",
        "# ============================================================\n",
        "\n",
        "checa_estacionariedade(mmm_marinha['consumo'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mQPb3Dg5Ev5D",
      "metadata": {
        "id": "mQPb3Dg5Ev5D"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Decomposição da série temporal em tendência, sazonalidade e resíduos\n",
        "# ============================================================\n",
        "\n",
        "# Aplica decomposição aditiva:\n",
        "# consumo = tendência + sazonalidade + resíduo\n",
        "# O parâmetro \"period=12\" assume sazonalidade anual (mensalidade em 12 meses).\n",
        "decomp = seasonal_decompose(\n",
        "    mmm_marinha.set_index('ano_mes')['consumo'],\n",
        "    model='additive',\n",
        "    period=12\n",
        ")\n",
        "\n",
        "# Gera visualização dos componentes:\n",
        "# - Série observada\n",
        "# - Tendência (variação de longo prazo)\n",
        "# - Sazonalidade (padrões que se repetem periodicamente)\n",
        "# - Resíduos (parte não explicada pelo modelo)\n",
        "fig = decomp.plot()\n",
        "fig.set_size_inches(20, 8)\n",
        "\n",
        "for ax in fig.axes:\n",
        "    ax.set_xlabel(\"Meses\")\n",
        "    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
        "    ax.xaxis.set_major_formatter(\n",
        "        plt.matplotlib.dates.DateFormatter(\"%b/%Y\")  # formato \"Jan/2020\"\n",
        "    )\n",
        "    ax.tick_params(axis=\"x\", rotation=45)\n",
        "\n",
        "plt.suptitle(\"Decomposição da Série Temporal (Modelo Aditivo)\", fontsize=14, y=1.02)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gUJDBObHEzh1",
      "metadata": {
        "id": "gUJDBObHEzh1"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Análise de Autocorrelação (ACF) e Autocorrelação Parcial (PACF)\n",
        "# ============================================================\n",
        "\n",
        "# Cria figura com dois subplots: um para ACF e outro para PACF\n",
        "fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
        "\n",
        "# Autocorrelação (ACF)\n",
        "# - Mede a correlação da série com suas próprias defasagens\n",
        "# - Útil para identificar dependência temporal e possíveis lags para ARIMA\n",
        "plot_acf(\n",
        "    mmm_marinha['consumo'].dropna(),  # remove NaNs antes do cálculo\n",
        "    lags=36,                           # número de defasagens a serem exibidas\n",
        "    ax=ax[0]\n",
        ")\n",
        "ax[0].set_title(\"Autocorrelação (ACF) - Consumo\")\n",
        "\n",
        "# Autocorrelação Parcial (PACF)\n",
        "# - Mede a correlação da série com uma defasagem específica,\n",
        "#   removendo efeitos das defasagens intermediárias\n",
        "# - Ajuda a identificar a ordem AR (p) em modelos ARIMA\n",
        "plot_pacf(\n",
        "    mmm_marinha['consumo'].dropna(),\n",
        "    lags=36,\n",
        "    ax=ax[1],\n",
        "    method='ywm'   # método de estimação robusto para PACF\n",
        ")\n",
        "ax[1].set_title(\"Autocorrelação Parcial (PACF) - Consumo\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0mYyWRosQH0C",
      "metadata": {
        "id": "0mYyWRosQH0C"
      },
      "source": [
        "### Teste de modelos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = mmm_marinha.iloc[:-12]\n",
        "test = mmm_marinha.iloc[-12:]"
      ],
      "metadata": {
        "id": "d8dtYUKvigHi"
      },
      "id": "d8dtYUKvigHi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "-07Wb9xwSs4S",
      "metadata": {
        "id": "-07Wb9xwSs4S"
      },
      "source": [
        "#### Previsão Naïve (critério de comparação)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80c6cb01",
      "metadata": {
        "id": "80c6cb01"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Previsão Naïve (Baseline)\n",
        "# ============================================================\n",
        "\n",
        "# A previsão \"naïve\" assume que o valor futuro é igual ao último valor observado.\n",
        "# Aqui usamos shift(1) para criar previsões deslocadas em 1 período.\n",
        "naive_forecast = mmm_marinha['consumo'].shift(1)\n",
        "\n",
        "# Calcula o MAE (Mean Absolute Error) como métrica de avaliação\n",
        "# - Excluímos o primeiro valor, pois a previsão para o primeiro ponto é NaN\n",
        "mae_naive = mean_absolute_error(\n",
        "    mmm_marinha['consumo'].iloc[1:],  # valores reais (a partir do segundo ponto)\n",
        "    naive_forecast.iloc[1:]            # valores previstos pela abordagem naïve\n",
        ")\n",
        "\n",
        "print('Baseline Naïve MAE:', mae_naive)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plota_resultados(\n",
        "    df=mmm_marinha,\n",
        "    x_col=\"ano_mes\",\n",
        "    y_col=\"consumo\",\n",
        "    title=\"Previsão temporal — Naïve\",\n",
        "    preds={\n",
        "        \"Naïve\": (test.ano_mes, naive_forecast[-12:])\n",
        "    }\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "yG6HgOAnylG7"
      },
      "id": "yG6HgOAnylG7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6vQyvOIWS05M",
      "metadata": {
        "id": "6vQyvOIWS05M"
      },
      "source": [
        "#### Previsão SARIMA (critério de comparação)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cbd40b6",
      "metadata": {
        "id": "3cbd40b6"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Definição do espaço de busca\n",
        "# ============================================================\n",
        "\n",
        "# Parâmetros do componente ARIMA (p, d, q)\n",
        "p = d = q = range(0, 3)  # valores de 0 a 2\n",
        "\n",
        "# Parâmetros do componente sazonal (P, D, Q, m)\n",
        "P = D = Q = range(0, 2)  # valores de 0 a 1\n",
        "m = 12                   # sazonalidade mensal (12 meses)\n",
        "\n",
        "# Todas as combinações possíveis\n",
        "pdq = list(itertools.product(p, d, q))                          # combinações de (p, d, q)\n",
        "seasonal_pdq = list(itertools.product(P, D, Q, [m]))            # combinações de (P, D, Q, m)\n",
        "\n",
        "# ============================================================\n",
        "# Busca pelo melhor modelo com base no critério AIC (Akaike Information Criterion)\n",
        "# ============================================================\n",
        "best_aic = np.inf                                               # inicializa com infinito\n",
        "best_order, best_seasonal = None, None\n",
        "best_model = None\n",
        "\n",
        "# Loop por todas as combinações possíveis de parâmetros\n",
        "for order in pdq:\n",
        "    for seasonal_order in seasonal_pdq:\n",
        "        try:\n",
        "            # Ajusta o modelo SARIMA\n",
        "            model = sm.tsa.statespace.SARIMAX(\n",
        "                mmm_marinha['consumo'],                         # série temporal\n",
        "                order=order,                                    # parâmetros (p, d, q)\n",
        "                seasonal_order=seasonal_order,                  # parâmetros sazonais (P, D, Q, m)\n",
        "                enforce_stationarity=False                      # não força estacionariedade\n",
        "            )\n",
        "            results = model.fit(disp=False)\n",
        "\n",
        "            if results.aic < best_aic:                          # Atualiza se o modelo atual tiver melhor AIC\n",
        "                best_aic = results.aic\n",
        "                best_order, best_seasonal = order, seasonal_order\n",
        "                best_model = results\n",
        "\n",
        "        except Exception:\n",
        "            continue                                            # Ignora combinações que não convergem para o caso de Decomposition error\n",
        "\n",
        "# ============================================================\n",
        "# Resultado final da busca\n",
        "# ============================================================\n",
        "print(\n",
        "    f'Melhor modelo SARIMA encontrado: '\n",
        "    f'order={best_order}, seasonal_order={best_seasonal}'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Criação do modelo SARIMA com os melhores hiperparâmetros encontrados\n",
        "# ============================================================\n",
        "\n",
        "model = SARIMAX(\n",
        "    train.consumo,\n",
        "    order=best_order,                                           # - order: parâmetros (p, d, q) para parte autorregressiva e de médias móveis\n",
        "    seasonal_order=best_seasonal,                               # - seasonal_order: parâmetros sazonais (P, D, Q, m)\n",
        "    enforce_stationarity=False                                  # - enforce_stationarity: desabilitado para maior flexibilidade\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# Ajuste do modelo aos dados de treino\n",
        "# ============================================================\n",
        "\n",
        "res = model.fit(disp=False, maxiter=500)                        # maxiter=500: limite de iterações para garantir a convergência\n",
        "\n",
        "# ============================================================\n",
        "# Exibição de métricas detalhadas do modelo ajustado\n",
        "# ============================================================\n",
        "\n",
        "print(res.summary().tables[1])                                  # a tabela traz os coeficientes e estatísticas de significância\n",
        "\n",
        "# ============================================================\n",
        "# Geração da previsão para o horizonte desejado (12 meses)\n",
        "# ============================================================\n",
        "\n",
        "pred = res.get_forecast(steps=12)\n",
        "y_pred = pred.predicted_mean  # valores previstos\n",
        "\n",
        "# Cálculo do viés (tendência de super ou subestimar os valores reais)\n",
        "bias = abs(test.consumo - y_pred).mean()\n",
        "\n",
        "# Ajuste da previsão removendo o viés médio\n",
        "# → melhora a coerência da previsão em relação aos valores observados\n",
        "y_pred_no_bias = y_pred - bias\n",
        "\n",
        "# Avaliação da performance com a métrica MAE\n",
        "# quanto menor, melhor a acurácia da previsão\n",
        "print(f\"SARIMA MAE: {mean_absolute_error(test['consumo'], y_pred_no_bias)}\")"
      ],
      "metadata": {
        "id": "Ye0h5frgOGgT"
      },
      "id": "Ye0h5frgOGgT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plota_resultados(\n",
        "    title=\"Previsão temporal — SARIMA\",\n",
        "    preds={\n",
        "        \"SARIMA\": (test.ano_mes, y_pred_no_bias)\n",
        "    }\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "K71MGq_bfNgz"
      },
      "id": "K71MGq_bfNgz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "qqIYoFrQTRrO",
      "metadata": {
        "id": "qqIYoFrQTRrO"
      },
      "source": [
        "#### Exponential Smoothing (critério de comparação)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4aec5d4b",
      "metadata": {
        "id": "4aec5d4b"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Configurações do modelo Holt-Winters\n",
        "# ============================================================\n",
        "\n",
        "hw_config = {\n",
        "    \"trend\": \"add\",                                             # modelo considera tendência aditiva (crescimento linear)\n",
        "    \"seasonal\": \"add\",                                          # sazonalidade aditiva (variações se somam à tendência)\n",
        "    \"seasonal_periods\": 12                                      # ciclo sazonal de 12 meses (dados mensais -> sazonalidade anual)\n",
        "}\n",
        "\n",
        "# ============================================================\n",
        "# Instanciação e ajuste do modelo Holt-Winters\n",
        "# ============================================================\n",
        "\n",
        "hw_model = ExponentialSmoothing(\n",
        "    train.consumo,                                              # série de treino\n",
        "    **hw_config                                                 # passa as configurações definidas acima\n",
        ").fit(\n",
        "    optimized=True,                                             # ajusta automaticamente os melhores parâmetros de suavização\n",
        "    use_brute=True                                              # força busca exaustiva para maior chance de encontrar parâmetros ótimos\n",
        ")\n",
        "\n",
        "\n",
        "forecast_horizon = 12                                           # horizonte de previsão (número de passos à frente)\n",
        "pred_hw = hw_model.forecast(steps=forecast_horizon)             # geração de previsões\n",
        "\n",
        "print(f\"Holt-Winters MAE: {mean_absolute_error(test['consumo'], pred_hw):.2f}\") # avaliação da previsão\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d782840",
      "metadata": {
        "id": "8d782840"
      },
      "outputs": [],
      "source": [
        "fig = plota_resultados(\n",
        "    title=\"Previsão temporal — ExponentialSmoothing\",\n",
        "    preds={\n",
        "        \"ExponentialSmoothing\": (test.ano_mes, pred_hw)\n",
        "    }\n",
        ")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Tfo6DUp-TBlL",
      "metadata": {
        "id": "Tfo6DUp-TBlL"
      },
      "source": [
        "#### Prophet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "678e5898",
      "metadata": {
        "id": "678e5898"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Preparação dos dados para Prophet\n",
        "# ============================================================\n",
        "prophet_df = (\n",
        "    mmm_marinha[['ano_mes', 'consumo']]                         # seleciona colunas relevantes\n",
        "    .rename(columns={'ano_mes': 'ds', 'consumo': 'y'})          # renomeia para padrão Prophet: ds -> data, y -> valor\n",
        "    .assign(ds=lambda d: pd.to_datetime(d['ds'], format='%m_%Y'))  # converte strings para datetime\n",
        "    .sort_values('ds')                                          # garante ordem cronológica\n",
        "    .reset_index(drop=True)                                     # reseta índice após ordenação\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# Configuração do modelo Prophet\n",
        "# ============================================================\n",
        "model_prophet = Prophet(\n",
        "    yearly_seasonality=True,                                    # ativa sazonalidade anual\n",
        "    weekly_seasonality=False,                                   # desativa sazonalidade semanal\n",
        "    daily_seasonality=False,                                    # desativa sazonalidade diária\n",
        "    seasonality_mode=\"additive\",                                # modelo aditivo (soma tendência + sazonalidade)\n",
        "    interval_width=0.95                                         # intervalo de confiança de 95% para previsão\n",
        ")\n",
        "\n",
        "# Ajuste do modelo aos dados de treino\n",
        "model_prophet.fit(prophet_df)\n",
        "\n",
        "# ============================================================\n",
        "# Criação do dataframe para previsão futura\n",
        "# ============================================================\n",
        "forecast_horizon = 12                                           # meses à frente\n",
        "future = model_prophet.make_future_dataframe(\n",
        "    periods=forecast_horizon,                                   # número de passos à frente\n",
        "    freq='M'                                                    # frequência mensal\n",
        ")\n",
        "\n",
        "# Geração das previsões\n",
        "forecast = model_prophet.predict(future)\n",
        "\n",
        "# ============================================================\n",
        "# Seleção das previsões correspondentes ao conjunto de teste\n",
        "# ============================================================\n",
        "test_dates = pd.to_datetime(test['ano_mes'], format='%m_%Y')    # datas do teste\n",
        "forecast_test = forecast.set_index('ds').loc[test_dates]        # filtra apenas o horizonte de teste\n",
        "\n",
        "y_true = test['consumo']                # valores reais\n",
        "prophet_y_pred = forecast_test['yhat']  # valores previstos\n",
        "\n",
        "# ============================================================\n",
        "# Avaliação do modelo\n",
        "# ============================================================\n",
        "mae = mean_absolute_error(y_true, prophet_y_pred)               # erro médio absoluto\n",
        "rmse = mean_squared_error(y_true, prophet_y_pred)               # raiz do erro quadrático médio\n",
        "\n",
        "print(f\"Prophet — MAE: {mae:.2f} | RMSE: {rmse:.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f7c19f3",
      "metadata": {
        "id": "2f7c19f3"
      },
      "outputs": [],
      "source": [
        "fig = plota_resultados(\n",
        "    title=\"Previsão temporal — Prophet\",\n",
        "    preds={\n",
        "        \"Prophet\": (test.ano_mes, prophet_y_pred)\n",
        "    }\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2toQJB6hTFbt",
      "metadata": {
        "id": "2toQJB6hTFbt"
      },
      "source": [
        "#### XGBoost regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b9e0724",
      "metadata": {
        "id": "0b9e0724"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Preparação de features para XGBoost\n",
        "# ============================================================\n",
        "xg_df = mmm_marinha[['consumo']].copy()                         # série alvo\n",
        "\n",
        "for lag in [1, 3, 6]:                                           # Criação de lags\n",
        "    xg_df[f'lag{lag}'] = xg_df['consumo'].shift(lag)            # adiciona colunas com valores passados da série (lags)\n",
        "\n",
        "\n",
        "for window in [3, 6]:                                           # Criação de médias móveis (rolling)\n",
        "    xg_df[f'rolling{window}'] = xg_df['consumo'].rolling(window).mean() # adiciona colunas com média móvel da série, para capturar tendências locais\n",
        "\n",
        "\n",
        "xg_df = xg_df.dropna().reset_index(drop=True)                   # Remove linhas com NaN gerados pelos lags e médias móveis e reseta índice\n",
        "\n",
        "# ============================================================\n",
        "# Separação treino / teste\n",
        "# ============================================================\n",
        "horizon = 12                                                    # últimos 12 períodos serão usados como teste\n",
        "train_xg, test_xg = xg_df.iloc[:-horizon], xg_df.iloc[-horizon:]\n",
        "\n",
        "X_train, y_train = train_xg.drop(columns=[\"consumo\"]), train_xg[\"consumo\"]  # features\n",
        "X_test, y_test   = test_xg.drop(columns=[\"consumo\"]), test_xg[\"consumo\"]    # target\n",
        "\n",
        "# ============================================================\n",
        "# Configuração do modelo XGBoost\n",
        "# ============================================================\n",
        "xgb_params = dict(\n",
        "    n_estimators=300,                                           # número de árvores\n",
        "    learning_rate=0.05,                                         # taxa de aprendizado\n",
        "    max_depth=5,                                                # profundidade máxima de cada árvore\n",
        "    subsample=0.8,                                              # amostragem de linhas para cada árvore\n",
        "    colsample_bytree=0.8,                                       # amostragem de colunas para cada árvore\n",
        "    random_state=42,                                            # reprodutibilidade\n",
        "    n_jobs=-1,                                                  # usa todos os núcleos disponíveis\n",
        "    objective=\"reg:squarederror\",                               # objetivo de regressão\n",
        "    verbosity=0                                                 # sem logs de treino\n",
        ")\n",
        "\n",
        "xgb_model = XGBRegressor(**xgb_params)\n",
        "\n",
        "# ============================================================\n",
        "# Treinamento do modelo\n",
        "# ============================================================\n",
        "\n",
        "xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False) # fornecendo eval_set apenas para monitoramento (não será usado na métrica final, evitando dat leakage)\n",
        "\n",
        "# ============================================================\n",
        "# Geração de previsões e avaliação\n",
        "# ============================================================\n",
        "pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "mae = mean_absolute_error(y_test, pred_xgb)                     # erro médio absoluto\n",
        "rmse = mean_squared_error(y_test, pred_xgb)                     # raiz do erro quadrático médio\n",
        "\n",
        "print(f\"XGBoost — MAE: {mae:.2f} | RMSE: {rmse:.2f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "610a55a9",
      "metadata": {
        "id": "610a55a9"
      },
      "outputs": [],
      "source": [
        "fig = plota_resultados(\n",
        "    title=\"Previsão temporal — XGBoost\",\n",
        "    preds={\n",
        "        \"XGBoost\": (test.ano_mes, pred_xgb)\n",
        "    }\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mV8L6DMLTW-T",
      "metadata": {
        "id": "mV8L6DMLTW-T"
      },
      "source": [
        "#### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a3bfe4c",
      "metadata": {
        "id": "1a3bfe4c"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Normalização dos dados da série temporal\n",
        "# ============================================================\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))                     # escalamos os dados para o intervalo [0,1] para ajudar a estabilizar o gradiente durante o treinamento da rede neural\n",
        "despesas_scaled = scaler.fit_transform(\n",
        "    mmm_marinha.consumo.values.reshape(-1, 1)                   # reshape para matriz 2D (requisito para o MinMaxScaler)\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# Função para criar janelas temporais (sequências)\n",
        "# ============================================================\n",
        "def create_sequences(data, window=12):\n",
        "    \"\"\"\n",
        "    Constrói sequências de tamanho 'window' para predição de séries temporais.\n",
        "\n",
        "    Args:\n",
        "        data (array): série temporal escalada\n",
        "        window (int): número de passos no histórico usados para prever o próximo valor\n",
        "\n",
        "    Returns:\n",
        "        X (array): entradas, cada linha é uma janela de 'window' passos\n",
        "        y (array): saídas, cada valor é o próximo ponto a ser previsto\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - window):\n",
        "        X.append(data[i : i + window])                          # pega dados em uma janela de tamanho 'window'\n",
        "        y.append(data[i + window])                              # o alvo é o valor logo após a janela\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X, y = create_sequences(despesas_scaled)                        # criação das sequências a partir da série normalizada\n",
        "\n",
        "# ============================================================\n",
        "# Separação treino / teste\n",
        "# ============================================================\n",
        "split = len(X) - 12                                             # últimos 12 pontos reservados para teste\n",
        "X_train, X_test = X[:split], X[split:]\n",
        "y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "print('Shape treino (X, y):', X_train.shape, y_train.shape)     # checando os formatos dos conjuntos criados para alimentar a rede neural\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd4973a6",
      "metadata": {
        "id": "dd4973a6"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Definição do modelo LSTM\n",
        "# ============================================================\n",
        "model = Sequential([\n",
        "\n",
        "    LSTM(128,                                                   # primeira camada LSTM com 128 neurônios\n",
        "         activation='relu',                                     # activation 'ReLu para garantir a não-linearidade\n",
        "         return_sequences=True,                                 # return_sequences para empilhar com outra LSTM\n",
        "         input_shape=(12, 1)),                                  # janela de 12 steps e uma feature, de acordo com o shape dos dados de entrada\n",
        "\n",
        "    BatchNormalization(),                                       # normaliza ativações, acelerando o treinamento e evitando explosão/desaparecimento do gradiente\n",
        "\n",
        "    LSTM(64,                                                    # segunda camada LSTM com 64 neurônios\n",
        "         activation='relu',\n",
        "         return_sequences=False),                               # return_sequences falso, pois é a última camada de LSTM, retornando apenas o último estado\n",
        "\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Dense(32, activation='relu'),                               # camadas densas totalmente conectadas para refinar padrões temporais\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(16, activation='relu'),\n",
        "\n",
        "    Dense(1)                                                    # saída com 1 neurônio: previsão de um único valor contínuo\n",
        "])\n",
        "\n",
        "# ============================================================\n",
        "# Compilação do modelo\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(                         # otimizador adam para um aprendizado adaptativo eficiente\n",
        "        learning_rate=1e-3,\n",
        "        beta_1=0.9,\n",
        "        beta_2=0.999),\n",
        "    loss='mae',                                                 # Função de perda: MAE (erro absoluto médio), comum em séries temporais\n",
        "    metrics=['mse']                                             # Métrica: MSE (erro quadrático médio), para avaliação adicional\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# Callbacks para controle de treinamento\n",
        "# ============================================================\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=200, restore_best_weights=True, verbose=1),  # para o treinamento cedo se não houver melhora na validação\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=200, min_lr=1e-5, verbose=1) # reduz a taxa de aprendizado se a validação estagnar\n",
        "]\n",
        "\n",
        "# ============================================================\n",
        "# Treinamento do modelo\n",
        "# ============================================================\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=1000,                                                # treinamento em 1000 épocas (pode ser interrompido antes via EarlyStopping)\n",
        "    batch_size=16,                                              # batch de tamanho 16 (quantidade de sequências que o modelo vê antes de cada atualização de gradiente)\n",
        "    validation_data=(X_test, y_test),                           # dados do conjunto de testes para validação\n",
        "    callbacks=callbacks,                                        # callbacks ativados para evitar desperdício de tempo no treinamento e devolver o melhor resultado conhecido\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ab14322",
      "metadata": {
        "id": "8ab14322"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Geração das previsões com o modelo LSTM\n",
        "# ============================================================\n",
        "y_pred = model.predict(X_test)                                  # obtém previsões no mesmo espaço escalado usado no treino\n",
        "\n",
        "# ============================================================\n",
        "# Inversão do escalonamento (voltar valores para escala original)\n",
        "# ============================================================\n",
        "\n",
        "y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))    # \"desnormalizando\" y_test para comparação com os resultados da predição do modelo\n",
        "y_pred_inv = scaler.inverse_transform(y_pred)                   # \"desnormalizando\" y_pred também para que eles retomem a amplitude dos dados originais\n",
        "\n",
        "# ============================================================\n",
        "# Avaliação do desempenho (MAE)\n",
        "# ============================================================\n",
        "\n",
        "mae_lstm = mean_absolute_error(y_test_inv, y_pred_inv)          # cálculo da média do erro absoluto entre valores reais e previstos, na escala original para interpretabilidade da métrica\n",
        "\n",
        "print('LSTM MAE:', mae_lstm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfc0090f",
      "metadata": {
        "id": "bfc0090f"
      },
      "outputs": [],
      "source": [
        "fig = plota_resultados(\n",
        "    title=\"Previsão temporal — LSTM\",\n",
        "    preds={\n",
        "        \"LSTM\": (test.ano_mes, y_pred_inv.flatten())\n",
        "    }\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5M6EqDdRTalL",
      "metadata": {
        "id": "5M6EqDdRTalL"
      },
      "source": [
        "#### Multistep LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdf8a44c",
      "metadata": {
        "id": "bdf8a44c"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Função para criar janelas de treino com previsão multi-step\n",
        "# ============================================================\n",
        "def create_sequences_multistep(data, window=12, horizon=12):\n",
        "    \"\"\"\n",
        "    Gera pares (X, y) para treinamento de modelos de séries temporais multistep.\n",
        "\n",
        "    Args:\n",
        "        data (array): série temporal escalada ou normalizada\n",
        "        window (int): número de períodos usados como entrada (janelas passadas)\n",
        "        horizon (int): número de períodos futuros a serem previstos\n",
        "\n",
        "    Returns:\n",
        "        X (np.array): sequências de entrada (amostras × janela × features)\n",
        "        y (np.array): valores futuros (amostras × horizonte)\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "\n",
        "    for i in range(len(data) - window - horizon + 1):           # percorre a série até onde é possível formar uma janela completa + horizonte\n",
        "        X.append(data[i:i+window])                              # janela de entrada (ex.: últimos 12 meses)\n",
        "        y.append(data[i+window:i+window+horizon].flatten())     # horizonte de saída (ex.: próximos 12 meses)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Criação dos conjuntos de dados\n",
        "# ============================================================\n",
        "\n",
        "window = 12                                                     # janela de 12 meses (1 ano de histórico para prever)\n",
        "horizon = 12                                                    # previsão de 12 meses à frente\n",
        "X, y = create_sequences_multistep(despesas_scaled, window, horizon)\n",
        "\n",
        "# ============================================================\n",
        "# Divisão treino/teste\n",
        "# ============================================================\n",
        "split = int(len(X) * 0.8)                                       # 80% treino, 20% teste\n",
        "X_train, X_test = X[:split], X[split:]\n",
        "y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "# ============================================================\n",
        "# Verificação dos shapes\n",
        "# ============================================================\n",
        "print('X_train shape:', X_train.shape)                          # (amostras, janela, features)\n",
        "print('y_train shape:', y_train.shape)                          # (amostras, horizonte)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d401850",
      "metadata": {
        "id": "0d401850"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Definição da arquitetura da rede LSTM para previsão multi-step\n",
        "# ============================================================\n",
        "model = Sequential([\n",
        "\n",
        "    LSTM(128,                                                   # primeira camada LSTM (captura padrões temporais mais longos)\n",
        "         activation='relu',\n",
        "         return_sequences=True,\n",
        "         input_shape=(window, 1)\n",
        "         ),\n",
        "\n",
        "    BatchNormalization(),                                       # normaliza a saída da camada LSTM para estabilizar o treinamento\n",
        "\n",
        "\n",
        "    LSTM(64,                                                    # segunda camada LSTM (captura padrões mais refinados, sem retornar sequência completa)\n",
        "         activation='relu',\n",
        "         return_sequences=False\n",
        "         ),\n",
        "\n",
        "    BatchNormalization(),\n",
        "\n",
        "\n",
        "    Dense(32, activation='relu'),                               # Camadas densas totalmente conectadas para refinar o aprendizado\n",
        "    Dense(32, activation='relu'),\n",
        "\n",
        "    # Camada de saída com dimensão igual ao horizonte de previsão\n",
        "    Dense(horizon)  # previsão multi-step (ex.: 12 passos à frente)\n",
        "])\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Compilação do modelo\n",
        "# ============================================================\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-2),  # otimizador Adam com taxa de aprendizado inicial mais alta\n",
        "    loss=\"mse\",    # erro quadrático médio (mais sensível a grandes desvios)\n",
        "    metrics=[\"mae\"]  # erro absoluto médio (interpretação direta em unidades originais)\n",
        ")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Callbacks para controle do treinamento\n",
        "# ============================================================\n",
        "callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor=\"val_loss\",        # monitora a perda de validação\n",
        "        patience=200,              # interrompe se não houver melhora após 200 épocas\n",
        "        restore_best_weights=True, # restaura os melhores pesos obtidos\n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\",        # reduz a taxa de aprendizado se o modelo \"empacar\"\n",
        "        factor=0.5,                # reduz pela metade\n",
        "        patience=200,              # espera 200 épocas sem melhora\n",
        "        min_lr=1e-5,               # limite mínimo de taxa de aprendizado\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Treinamento do modelo\n",
        "# ============================================================\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=2000,                   # número máximo de épocas (early stopping pode parar antes)\n",
        "    batch_size=16,                 # tamanho do lote (menor => mais ruído, maior => gradiente mais estável)\n",
        "    validation_data=(X_test, y_test), # validação em dados de teste\n",
        "    callbacks=callbacks,           # usa callbacks definidos acima\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07ef65ad",
      "metadata": {
        "id": "07ef65ad"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Geração de previsões com o modelo treinado\n",
        "# ============================================================\n",
        "y_pred = model.predict(X_test)                                  # previsões para o conjunto de teste\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Inversão da normalização\n",
        "# ============================================================\n",
        "y_test_inv = scaler.inverse_transform(y_test)                   # valores reais (teste)\n",
        "y_pred_inv = scaler.inverse_transform(y_pred)                   # valores previstos\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Avaliação da performance\n",
        "# ============================================================\n",
        "mae_lstm_multi = mean_absolute_error(y_test_inv.flatten(), y_pred_inv.flatten())\n",
        "print('LSTM Multi-step MAE:', mae_lstm_multi)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a3ae83a",
      "metadata": {
        "id": "5a3ae83a"
      },
      "outputs": [],
      "source": [
        "fig = plota_resultados(\n",
        "    title=\"Previsão temporal — Multi-step LSTM\",\n",
        "    preds={\n",
        "        \"Multi-step LSTM\": (test.ano_mes, y_pred_inv[-1])\n",
        "    }\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D1MgfHeFXe6s",
      "metadata": {
        "id": "D1MgfHeFXe6s"
      },
      "source": [
        "#### N-Beats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6cda0ff",
      "metadata": {
        "id": "e6cda0ff"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Preparação dos dados para o N-BEATS\n",
        "# ============================================================\n",
        "train['item_id'] = 'mnc'                                        # identificador único da série temporal\n",
        "nbeats_data = train.rename(                                     # o NeuralForecast exige colunas com nomes específicos\n",
        "    columns={\n",
        "        'item_id': 'unique_id',                                 # unique_id: identifica a série (mesmo que haja apenas uma)\n",
        "        'ano_mes': 'ds',                                        # ds: datas (formato datetime)\n",
        "        'consumo': 'y'                                          # y: valores observados (variável alvo)\n",
        "        }\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# Configuração do modelo N-BEATS\n",
        "# ============================================================\n",
        "model = NBEATS(\n",
        "    h=12,                                                       # h: horizonte de previsão (quantos passos à frente prever)\n",
        "    input_size=36,                                              # input_size: quantidade de observações usadas como entrada\n",
        "    stack_types=['seasonality', 'identity', 'trend', 'identity'], # stack_types: tipos de blocos (tendência, sazonalidade, identidade)\n",
        "    n_blocks=[3, 3, 3, 2],                                      # n_blocks: número de blocos em cada pilha\n",
        "    activation='ReLU',\n",
        "\n",
        "    learning_rate=1e-3,                                         # learning_rate: taxa de aprendizado\n",
        "    num_lr_decays=3,                                            # num_lr_decays: número de reduções de LR automáticas\n",
        "    batch_size=16,                                              # batch_size: tamanho do lote (impacta velocidade/estabilidade do treino)\n",
        "    scaler_type='robust',                                       # scaler_type: tipo de normalização (\"robust\" lida melhor com outliers)\n",
        "\n",
        "    max_steps=1000,                                             # max_steps: limite de iterações no treinamento\n",
        "    val_check_steps=10,                                         # val_check_steps: frequência de checagem no conjunto de validação\n",
        "    early_stop_patience_steps=20                                # early_stop_patience_steps: paciência para early stopping\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# Treinamento do modelo\n",
        "# ============================================================\n",
        "nbeats_forecast = NeuralForecast(models=[model], freq='M')      # freq='M' indica que a série é mensal\n",
        "nbeats_forecast.fit(df=nbeats_data, val_size=12)                # val_size=12 → reserva de 12 meses para validação\n",
        "\n",
        "# ============================================================\n",
        "# Geração das previsões\n",
        "# ============================================================\n",
        "y_pred_nbeats = nbeats_forecast.predict()                       # Retorna as previsões multi-step para o horizonte definido\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plota_resultados(\n",
        "    title=\"Previsão temporal — N-Beats\",\n",
        "    preds={\n",
        "        \"N-Beats\": (test.ano_mes, y_pred_nbeats.NBEATS.values.flatten())\n",
        "    }\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "vTEP6ZTytjB8"
      },
      "id": "vTEP6ZTytjB8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Autogluon"
      ],
      "metadata": {
        "id": "msJ331VFVejD"
      },
      "id": "msJ331VFVejD"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Preparação dos dados para o AutoGluon\n",
        "# ============================================================\n",
        "autogluon_data = nbeats_data.rename(                            # reaproveitando os dados utilizados para treinamento do NBeats (pois são de formato parecido)\n",
        "    columns={                                                   # o AutoGluon espera as seguintes colunas:\n",
        "        'unique_id': 'item_id',                                 # item_id: identifica a série temporal\n",
        "        'ds': 'timestamp',                                      # timestamp: data/hora dos registros (datetime)\n",
        "        'y': 'target'                                           # target: valores observados (variável alvo)\n",
        "        }\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# Definição do horizonte de previsão\n",
        "# ============================================================\n",
        "prediction_length = 12                                          # prediction_length: número de passos à frente para prever\n",
        "\n",
        "train_df = autogluon_data.iloc[:-prediction_length].copy()      # separação simples entre treino e teste\n",
        "test_df  = autogluon_data.iloc[-prediction_length:].copy()\n",
        "\n",
        "# ============================================================\n",
        "# Criação de subconjuntos para treino/validação\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "val_size = prediction_length                                    # val_size: tamanho do conjunto de validação\n",
        "train_for_fit = autogluon_data.iloc[:-val_size].copy()\n",
        "tune_for_fit  = autogluon_data.iloc[-val_size - prediction_length : -prediction_length].copy()  # tune_for_fit: dados imediatamente antes do teste (usados em ajuste fino)\n",
        "\n",
        "# ============================================================\n",
        "# Instanciação do AutoGluon TimeSeriesPredictor\n",
        "# ============================================================\n",
        "predictor = TimeSeriesPredictor(\n",
        "    target='target',                                            # target: variável a ser prevista\n",
        "    prediction_length=prediction_length,                        # prediction_length: horizonte da previsão\n",
        "    eval_metric='MAE'                                           # eval_metric: métrica de avaliação primária\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# Treinamento do AutoGluon\n",
        "# ============================================================\n",
        "predictor.fit(\n",
        "    train_data=train_for_fit,\n",
        "    presets='best_quality',                                     # presets='best_quality': busca modelos mais robustos (mesmo que mais lentos)\n",
        "    time_limit=600,                                             # time_limit: limite de tempo em segundos\n",
        "    verbosity=2                                                 # verbosity: nível de detalhamento do log\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# Geração das previsões\n",
        "# ============================================================\n",
        "predictions = predictor.predict(autogluon_data)                 # aqui usamos o conjunto completo e o AutoGluon automaticamente deduz o ponto de corte a partor do 'prediction_length'\n"
      ],
      "metadata": {
        "id": "iUGj0T7uVjOo"
      },
      "id": "iUGj0T7uVjOo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plota_resultados(\n",
        "    title=\"Previsão temporal — Autogluon\",\n",
        "    preds={\n",
        "        \"Autogluon\": (test.ano_mes, predictions['mean'].values)\n",
        "    }\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "VgX3VA0TWujd"
      },
      "id": "VgX3VA0TWujd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "MmTXRrVdTith",
      "metadata": {
        "id": "MmTXRrVdTith"
      },
      "source": [
        "### Seleção do melhor modelo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''# #@title Comparação de modelos\n",
        "# import ipywidgets as widgets\n",
        "# from IPython.display import display, clear_output\n",
        "\n",
        "# # Dropdowns interativos\n",
        "# model_dropdown = widgets.Dropdown(\n",
        "#     options=[\"SARIMAX\", \"Prophet\", \"XGBoost\", \"ExponentialSmoothing\", \"LSTM\", \"N-BEATS\", \"AutoGluon\"],\n",
        "#     value=\"LSTM\",\n",
        "#     description=\"Modelo:\"\n",
        "# )\n",
        "\n",
        "# test_dropdown = widgets.Dropdown(\n",
        "#     options=[\"A\", \"B\", \"C\"],\n",
        "#     value=\"A\",\n",
        "#     description=\"Teste:\"\n",
        "# )\n",
        "\n",
        "# output = widgets.Output()\n",
        "\n",
        "# def update_plot(change):\n",
        "#     with output:\n",
        "#         clear_output()\n",
        "#         model = model_dropdown.value\n",
        "#         test_set = test_dropdown.value\n",
        "\n",
        "#         print(f\"📊 Modelo selecionado: {model}\")\n",
        "#         print(f\"📂 Conjunto de teste : {test_set}\")\n",
        "\n",
        "#         # Seleção de previsão\n",
        "#         if model == \"LSTM\":\n",
        "#             y_pred = pred_lstm\n",
        "#         elif model == \"XGBoost\":\n",
        "#             y_pred = pred_xgb\n",
        "#         elif model == \"Prophet\":\n",
        "#             y_pred = forecast_test['yhat']\n",
        "#         elif model == \"SARIMAX\":\n",
        "#             y_pred = pred_sarimax\n",
        "#         elif model == \"ExponentialSmoothing\":\n",
        "#             y_pred = pred_hw\n",
        "#         elif model == \"N-BEATS\":\n",
        "#             y_pred = y_pred_nbeats\n",
        "#         elif model == \"AutoGluon\":\n",
        "#             y_pred = y_pred_autogluon\n",
        "\n",
        "#         # Plot interativo\n",
        "#         fig = grafico_base(f\"Previsão temporal com {model}\")\n",
        "#         fig.add_scatter(x=test.ano_mes, y=y_pred, mode=\"lines+markers\", name=f\"{model} Forecast\")\n",
        "#         fig.show()\n",
        "\n",
        "# # Ligando evento\n",
        "# model_dropdown.observe(update_plot, names=\"value\")\n",
        "# test_dropdown.observe(update_plot, names=\"value\")\n",
        "\n",
        "# display(model_dropdown, test_dropdown, output)\n",
        "\n",
        "# # Render inicial\n",
        "# update_plot(None)\n",
        "'''"
      ],
      "metadata": {
        "id": "_Y2-2hodalJy"
      },
      "id": "_Y2-2hodalJy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "A4JcV4jXYCrE",
      "metadata": {
        "id": "A4JcV4jXYCrE"
      },
      "source": [
        "### Conclusões"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "elR7bye6YFZn",
      "metadata": {
        "id": "elR7bye6YFZn"
      },
      "source": [
        "## Trabalhos futuros"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}